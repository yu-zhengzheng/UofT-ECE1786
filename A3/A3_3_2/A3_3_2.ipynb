{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datasets\n",
    "# The code below is needed for using Google Colab, so un comment this if that is what you're using\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# The code below is also needed for using Google Colab\n",
    "# BEFORE executing this, you must place the mingpt folder supplied in the assignment\n",
    "# your google drive, within the folder \"Colab Notebooks\"\n",
    "#\n",
    "# It mounts and changes into the folder that contains mingpt, which you must upload to google drive\n",
    "# So un-comment it if you've uploaded mingpt to your google drive, into the  \"Colab Notebooks\" folder\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/ECE\\ 1786/1786A4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.bpe import BPETokenizer\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(1234)\n",
    "\"\"\"\n",
    "Prepare the dataset to train the Language Model (LM)\n",
    "This implementation splits the sentences and so doesn't create training\n",
    "examples that cross sentences.\n",
    "\n",
    "This code is set so that it uses one of two possible datasets, which were also used in Assignment 1:\n",
    "SmallSimpleCorpus.txt or LargerCorpus.txt\n",
    "\n",
    "Arguments:\n",
    "            ds_choice: str. \"small\" or \"large\". (i.e. selects which of the two datasets)\n",
    "            split: str. \"train\" or \"test\".\n",
    "            truncation: int. If -1: no truncation on sentences. Otherwise: truncate to this specific length.\n",
    "\"\"\"\n",
    "\n",
    "class LanguageModelingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds_choice=\"small\", split=\"train\", truncation=-1):\n",
    "\n",
    "        base_path = \"./\"\n",
    "        fn = {\"small\": \"SmallSimpleCorpus.txt\", \"large\": \"LargerCorpus.txt\"}\n",
    "        self.ds_choice = ds_choice\n",
    "        self.truncation = truncation  # int. If -1, then\n",
    "        text = Path(base_path, fn[ds_choice]).read_text(encoding='utf-8')\n",
    "        if ds_choice == \"large\":\n",
    "            # Remove the newline char in the middle of sentences\n",
    "            # The \"paragraph splitting\" newlines appear to be \\n\\n -- remove the duplications there\n",
    "            text = text.replace(\"\\n\\n\", \"$$^^$$\").replace(\"\\n\", \" \").replace(\"$$^^$$\", \"\\n\")\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Train / test split\n",
    "        train, val = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "        if split == \"train\":\n",
    "            raw_data = train\n",
    "        else:\n",
    "            raw_data = val\n",
    "\n",
    "        # Tokenize\n",
    "        self.tokenizer = BPETokenizer()\n",
    "        self.data = []  # List of 1-d pytorch tensor\n",
    "        for sent in raw_data:\n",
    "            tokenized = self.tokenizer(sent).view(-1)  # pytorch tensor\n",
    "            if truncation >= 0:\n",
    "                self.data.append(tokenized[:truncation])\n",
    "            else:\n",
    "                self.data.append(tokenized)\n",
    "            #print(tokenized)\n",
    "        # Count some items\n",
    "        self.max_sentence_length = np.max([len(d) for d in self.data])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer),\n",
    "        but actually, only a small number of vocab is used, especially for the small text.\n",
    "        \"\"\"\n",
    "        return 50257\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The output should be a tuple x and y, both as pytorch tensors.\n",
    "        Please refer to the `run()` method in the mingpt/trainer.py script for\n",
    "        how the x and y are going to be used.\n",
    "        \"\"\"\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return (x, y)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        \"\"\"\n",
    "        block_size is the size at which lines are truncated to ensure they are equal-length.\n",
    "        \"\"\"\n",
    "        return self.max_sentence_length\n",
    "\n",
    "# Instantiate the Training Dataset\n",
    "#train_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"train\")  # use this for the short corpus\n",
    "train_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"train\", truncation=512) #use this for long\n",
    "\n",
    "# Instantiate a Validation Dataset (this is only really needed for the fine-tune task, not the LM task)\n",
    "#val_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"validation\")\n",
    "val_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"validation\", truncation=512)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T22:24:32.035329Z",
     "start_time": "2023-10-22T22:24:29.402965200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'contains no wit , only labored gags ', 'label': 0, 'idx': 1}\n",
      "<class 'list'>\n",
      "that loves its characters and communicates something rather beautiful about human nature \n",
      "no need to stratify!\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "sst=datasets.load_dataset(\"glue\", \"sst2\")\n",
    "print(sst[\"train\"][1])\n",
    "data=sst[\"train\"][0:1200]\n",
    "X=data['sentence']\n",
    "Y = data['label']\n",
    "print(type(X))\n",
    "print(X[2])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, stratify=None, random_state=42)\n",
    "diff=sum(Y_train)-(sum(Y_train)+sum(Y_test))*0.8\n",
    "if diff<1:\n",
    "    print(\"no need to stratify!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T22:24:33.628955500Z",
     "start_time": "2023-10-22T22:24:32.037326800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "\n",
    "    def __init__(self,data,label, truncation=-1):\n",
    "\n",
    "        for i in data:\n",
    "            sentences = sent_tokenize(i)\n",
    "\n",
    "        # Tokenize\n",
    "        self.tokenizer = BPETokenizer()\n",
    "        self.data = []  # List of 1-d pytorch tensor\n",
    "        self.label = []\n",
    "        for sent in data:\n",
    "            tokenized = self.tokenizer(sent).view(-1)  # pytorch tensor\n",
    "            self.data.append(tokenized)\n",
    "        for lab in label:\n",
    "            self.label.append([torch.tensor(lab).float(),torch.tensor(1-lab).float()])\n",
    "        # Count some items\n",
    "        try:\n",
    "            self.max_sentence_length = np.max([len(d) for d in self.data])\n",
    "            #ValueError: zero-size array to reduction operation maximum which has no identity\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer),\n",
    "        but actually, only a small number of vocab is used, especially for the small text.\n",
    "        \"\"\"\n",
    "        return 50257\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The output should be a tuple x and y, both as pytorch tensors.\n",
    "        Please refer to the `run()` method in the mingpt/trainer.py script for\n",
    "        how the x and y are going to be used.\n",
    "        \"\"\"\n",
    "        x = self.data[idx]\n",
    "        y = self.label[idx]\n",
    "        return (x, y)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        \"\"\"\n",
    "        block_size is the size at which lines are truncated to ensure they are equal-length.\n",
    "        \"\"\"\n",
    "        return self.max_sentence_length\n",
    "\n",
    "# Instantiate the Training Dataset\n",
    "#train_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"train\", truncation=512) #use this for long\n",
    "train_dataset = ClassifierDataset(X_train,Y_train, truncation=512)\n",
    "# Instantiate a Validation Dataset (this is only really needed for the fine-tune task, not the LM task)\n",
    "val_dataset = ClassifierDataset(X_test,Y_test, truncation=512)\n",
    "print(train_dataset.get_block_size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T22:24:33.890702700Z",
     "start_time": "2023-10-22T22:24:33.631952700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10134,   262,  8754,    12, 45525,  1254,   286,   257,  3931,    12,\n",
      "        16544,  7401,   905,  1058, 37205,  3194,   837,   739,   260,   258,\n",
      "          945,   276,   837, 40647, 37515,   290,   220]) [tensor(0.), tensor(1.)]\n",
      "X:  has the thrown-together feel of a summer-camp talent show : hastily written , underrehearsed , arbitrarily plotted and \n",
      "Y:  [tensor(0.), tensor(1.)]\n",
      "number of parameters: 2.52M\n",
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "def lm_collate_fn(batch, device):\n",
    "    x = [item[0] for item in batch]  # List (len B) of varying lengths\n",
    "    y = [item[1] for item in batch]  # List (len B) of the same lengths as x\n",
    "    maxlen = max([len(s) for s in x])\n",
    "    maxen=512#to set make it align with LM\n",
    "    padded_x = []\n",
    "    for sx in x:\n",
    "        padded_x.append(torch.cat([sx, torch.ones(maxlen - len(sx))]))\n",
    "    return torch.stack(padded_x).long().to(device), torch.tensor(y)\n",
    "\n",
    "# Print out an example of the data - this is processed more once it reaches lm_collate_fn (above)\n",
    "x,y = train_dataset[5]\n",
    "print(x, y)\n",
    "print(\"X: \",train_dataset.tokenizer.decode(x))\n",
    "print(\"Y: \",y)\n",
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = 512#train_dataset.get_block_size()\n",
    "model_config.n_classification_class = 2\n",
    "model = GPT(model_config)\n",
    "# Create a Trainer object and set the core hyper-parameters\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 1e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 100000  # For small corpus: 3000 iterations is plenty. For large corpus: 100000 iterations is needed\n",
    "train_config.num_workers = 0\n",
    "train_config.batch_size = 4    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n",
    "trainer = Trainer(train_config, model, train_dataset, val_dataset,downstream_finetune=True, collate_fn=lm_collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:27:07.903953100Z",
     "start_time": "2023-10-23T02:27:07.815235800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "if True:\n",
    "    modelsavename= \"mint.pt\"\n",
    "    # The code below shows how to reload the model from the saved file; is useful things that take long to train\n",
    "    model.load_state_dict(torch.load(modelsavename))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:27:08.007740600Z",
     "start_time": "2023-10-23T02:27:07.965858100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 19.48191\n",
      "iter_dt 9.03ms; iter 100: train loss 3.32920\n",
      "iter_dt 8.05ms; iter 200: train loss 1.98931\n",
      "iter_dt 8.95ms; iter 300: train loss 0.85473\n",
      "iter_dt 11.01ms; iter 400: train loss 0.28399\n",
      "iter_dt 8.98ms; iter 500: train loss 0.74150\n",
      "iter_dt 9.00ms; iter 600: train loss 0.84049\n",
      "iter_dt 9.66ms; iter 700: train loss 0.35652\n",
      "iter_dt 10.01ms; iter 800: train loss 0.73660\n",
      "iter_dt 10.00ms; iter 900: train loss 0.45420\n",
      "iter_dt 25.28ms; iter 1000: train loss 0.34762\n",
      "iter_dt 23.20ms; iter 1100: train loss 0.68618\n",
      "iter_dt 19.98ms; iter 1200: train loss 0.44779\n",
      "iter_dt 8.00ms; iter 1300: train loss 0.37161\n",
      "iter_dt 9.91ms; iter 1400: train loss 0.21037\n",
      "iter_dt 10.37ms; iter 1500: train loss 0.31186\n",
      "iter_dt 28.04ms; iter 1600: train loss 0.14913\n",
      "iter_dt 9.14ms; iter 1700: train loss 0.41132\n",
      "iter_dt 10.02ms; iter 1800: train loss 0.13296\n",
      "iter_dt 8.49ms; iter 1900: train loss 0.21360\n",
      "iter_dt 9.01ms; iter 2000: train loss 0.21865\n",
      "iter_dt 7.98ms; iter 2100: train loss 0.09780\n",
      "iter_dt 10.03ms; iter 2200: train loss 0.24629\n",
      "iter_dt 9.97ms; iter 2300: train loss 0.28883\n",
      "iter_dt 9.00ms; iter 2400: train loss 0.33185\n",
      "iter_dt 9.00ms; iter 2500: train loss 0.20583\n",
      "iter_dt 8.00ms; iter 2600: train loss 0.31859\n",
      "iter_dt 8.00ms; iter 2700: train loss 0.32575\n",
      "iter_dt 8.87ms; iter 2800: train loss 0.12775\n",
      "iter_dt 9.64ms; iter 2900: train loss 0.06572\n",
      "iter_dt 10.00ms; iter 3000: train loss 0.45160\n",
      "iter_dt 9.00ms; iter 3100: train loss 0.20914\n",
      "iter_dt 9.00ms; iter 3200: train loss 0.17276\n",
      "iter_dt 7.97ms; iter 3300: train loss 0.10210\n",
      "iter_dt 9.92ms; iter 3400: train loss 0.09044\n",
      "iter_dt 10.00ms; iter 3500: train loss 0.08950\n",
      "iter_dt 8.98ms; iter 3600: train loss 0.10737\n",
      "iter_dt 8.00ms; iter 3700: train loss 0.04488\n",
      "iter_dt 9.11ms; iter 3800: train loss 0.09685\n",
      "iter_dt 9.69ms; iter 3900: train loss 0.02252\n",
      "iter_dt 9.06ms; iter 4000: train loss 0.43908\n",
      "iter_dt 9.97ms; iter 4100: train loss 0.00889\n",
      "iter_dt 9.10ms; iter 4200: train loss 0.32990\n",
      "iter_dt 7.99ms; iter 4300: train loss 0.09532\n",
      "iter_dt 9.00ms; iter 4400: train loss 0.10259\n",
      "iter_dt 9.76ms; iter 4500: train loss 0.18925\n",
      "iter_dt 9.61ms; iter 4600: train loss 0.05876\n",
      "iter_dt 8.99ms; iter 4700: train loss 0.01252\n",
      "iter_dt 9.15ms; iter 4800: train loss 0.07095\n",
      "iter_dt 10.02ms; iter 4900: train loss 0.05723\n",
      "iter_dt 9.90ms; iter 5000: train loss 0.14256\n",
      "iter_dt 8.90ms; iter 5100: train loss 0.07480\n",
      "iter_dt 9.00ms; iter 5200: train loss 0.01121\n",
      "iter_dt 8.03ms; iter 5300: train loss 0.02555\n",
      "iter_dt 9.00ms; iter 5400: train loss 0.14508\n",
      "iter_dt 9.02ms; iter 5500: train loss 0.05799\n",
      "iter_dt 9.00ms; iter 5600: train loss 0.01988\n",
      "iter_dt 7.97ms; iter 5700: train loss 0.08138\n",
      "iter_dt 9.00ms; iter 5800: train loss 0.01293\n",
      "iter_dt 9.03ms; iter 5900: train loss 0.00487\n",
      "iter_dt 0.00ms; iter 6000: train loss 0.01257\n",
      "iter_dt 10.02ms; iter 6100: train loss 0.02267\n",
      "iter_dt 9.00ms; iter 6200: train loss 0.11150\n",
      "iter_dt 9.01ms; iter 6300: train loss 0.02071\n",
      "iter_dt 9.00ms; iter 6400: train loss 0.00908\n",
      "iter_dt 8.00ms; iter 6500: train loss 0.17720\n",
      "iter_dt 8.92ms; iter 6600: train loss 0.02645\n",
      "iter_dt 8.99ms; iter 6700: train loss 0.01333\n",
      "iter_dt 10.01ms; iter 6800: train loss 0.06074\n",
      "iter_dt 9.02ms; iter 6900: train loss 0.00721\n",
      "iter_dt 8.91ms; iter 7000: train loss 0.01055\n",
      "iter_dt 8.02ms; iter 7100: train loss 0.00341\n",
      "iter_dt 7.98ms; iter 7200: train loss 0.00459\n",
      "iter_dt 8.01ms; iter 7300: train loss 0.06558\n",
      "iter_dt 8.99ms; iter 7400: train loss 0.04278\n",
      "iter_dt 9.00ms; iter 7500: train loss 0.02670\n",
      "iter_dt 8.88ms; iter 7600: train loss 0.01728\n",
      "iter_dt 8.73ms; iter 7700: train loss 0.17146\n",
      "iter_dt 8.01ms; iter 7800: train loss 0.01091\n",
      "iter_dt 9.00ms; iter 7900: train loss 0.02043\n",
      "iter_dt 9.00ms; iter 8000: train loss 0.01608\n",
      "iter_dt 8.96ms; iter 8100: train loss 0.00693\n",
      "iter_dt 9.00ms; iter 8200: train loss 0.00981\n",
      "iter_dt 10.00ms; iter 8300: train loss 0.01266\n",
      "iter_dt 9.51ms; iter 8400: train loss 0.01315\n",
      "iter_dt 8.97ms; iter 8500: train loss 0.01345\n",
      "iter_dt 9.00ms; iter 8600: train loss 0.01758\n",
      "iter_dt 9.88ms; iter 8700: train loss 0.01412\n",
      "iter_dt 9.00ms; iter 8800: train loss 0.00883\n",
      "iter_dt 9.19ms; iter 8900: train loss 0.03619\n",
      "iter_dt 8.01ms; iter 9000: train loss 0.00970\n",
      "iter_dt 8.98ms; iter 9100: train loss 0.12438\n",
      "iter_dt 9.02ms; iter 9200: train loss 0.00463\n",
      "iter_dt 9.93ms; iter 9300: train loss 0.00635\n",
      "iter_dt 9.02ms; iter 9400: train loss 0.01764\n",
      "iter_dt 9.61ms; iter 9500: train loss 0.06098\n",
      "iter_dt 9.00ms; iter 9600: train loss 0.00595\n",
      "iter_dt 9.13ms; iter 9700: train loss 0.03585\n",
      "iter_dt 8.98ms; iter 9800: train loss 0.01105\n",
      "iter_dt 8.01ms; iter 9900: train loss 0.22946\n",
      "iter_dt 10.00ms; iter 10000: train loss 0.03419\n",
      "iter_dt 9.01ms; iter 10100: train loss 0.23074\n",
      "iter_dt 8.00ms; iter 10200: train loss 0.00389\n",
      "iter_dt 9.00ms; iter 10300: train loss 0.00049\n",
      "iter_dt 9.57ms; iter 10400: train loss 0.01544\n",
      "iter_dt 9.10ms; iter 10500: train loss 0.01295\n",
      "iter_dt 8.98ms; iter 10600: train loss 0.00782\n",
      "iter_dt 8.99ms; iter 10700: train loss 0.00401\n",
      "iter_dt 8.00ms; iter 10800: train loss 0.00569\n",
      "iter_dt 9.00ms; iter 10900: train loss 0.00227\n",
      "iter_dt 9.01ms; iter 11000: train loss 0.00407\n",
      "iter_dt 8.51ms; iter 11100: train loss 0.00553\n",
      "iter_dt 9.02ms; iter 11200: train loss 0.00546\n",
      "iter_dt 9.70ms; iter 11300: train loss 0.00589\n",
      "iter_dt 10.00ms; iter 11400: train loss 0.00456\n",
      "iter_dt 9.07ms; iter 11500: train loss 0.00408\n",
      "iter_dt 10.00ms; iter 11600: train loss 0.00520\n",
      "iter_dt 8.96ms; iter 11700: train loss 0.00472\n",
      "iter_dt 9.98ms; iter 11800: train loss 0.00300\n",
      "iter_dt 9.06ms; iter 11900: train loss 0.01536\n",
      "iter_dt 9.01ms; iter 12000: train loss 0.01496\n",
      "iter_dt 9.78ms; iter 12100: train loss 0.02141\n",
      "iter_dt 0.00ms; iter 12200: train loss 0.00108\n",
      "iter_dt 9.03ms; iter 12300: train loss 0.00583\n",
      "iter_dt 9.03ms; iter 12400: train loss 0.01012\n",
      "iter_dt 10.00ms; iter 12500: train loss 0.00352\n",
      "iter_dt 9.00ms; iter 12600: train loss 0.00598\n",
      "iter_dt 9.98ms; iter 12700: train loss 0.00882\n",
      "iter_dt 8.98ms; iter 12800: train loss 0.00180\n",
      "iter_dt 9.09ms; iter 12900: train loss 0.01063\n",
      "iter_dt 9.01ms; iter 13000: train loss 0.00455\n",
      "iter_dt 9.00ms; iter 13100: train loss 0.00274\n",
      "iter_dt 0.00ms; iter 13200: train loss 0.00198\n",
      "iter_dt 9.11ms; iter 13300: train loss 0.00363\n",
      "iter_dt 9.00ms; iter 13400: train loss 0.00271\n",
      "iter_dt 10.02ms; iter 13500: train loss 0.00043\n",
      "iter_dt 9.00ms; iter 13600: train loss 0.00605\n",
      "iter_dt 10.00ms; iter 13700: train loss 0.00202\n",
      "iter_dt 4.07ms; iter 13800: train loss 0.00203\n",
      "iter_dt 8.93ms; iter 13900: train loss 0.00242\n",
      "iter_dt 8.98ms; iter 14000: train loss 0.00145\n",
      "iter_dt 10.06ms; iter 14100: train loss 0.00404\n",
      "iter_dt 8.66ms; iter 14200: train loss 0.00140\n",
      "iter_dt 2.06ms; iter 14300: train loss 0.00557\n",
      "iter_dt 10.00ms; iter 14400: train loss 0.00690\n",
      "iter_dt 9.76ms; iter 14500: train loss 0.00112\n",
      "iter_dt 9.00ms; iter 14600: train loss 0.00374\n",
      "iter_dt 8.98ms; iter 14700: train loss 0.00292\n",
      "iter_dt 9.00ms; iter 14800: train loss 0.00622\n",
      "iter_dt 9.01ms; iter 14900: train loss 0.00609\n",
      "iter_dt 9.01ms; iter 15000: train loss 0.00150\n",
      "iter_dt 9.01ms; iter 15100: train loss 0.00037\n",
      "iter_dt 8.54ms; iter 15200: train loss 0.00302\n",
      "iter_dt 9.00ms; iter 15300: train loss 0.00157\n",
      "iter_dt 10.00ms; iter 15400: train loss 0.00206\n",
      "iter_dt 2.08ms; iter 15500: train loss 0.00231\n",
      "iter_dt 8.99ms; iter 15600: train loss 0.00229\n",
      "iter_dt 8.80ms; iter 15700: train loss 0.00152\n",
      "iter_dt 8.00ms; iter 15800: train loss 0.00260\n",
      "iter_dt 8.98ms; iter 15900: train loss 0.00342\n",
      "iter_dt 0.00ms; iter 16000: train loss 0.00498\n",
      "iter_dt 10.00ms; iter 16100: train loss 0.00231\n",
      "iter_dt 9.46ms; iter 16200: train loss 0.00533\n",
      "iter_dt 9.00ms; iter 16300: train loss 0.00327\n",
      "iter_dt 9.14ms; iter 16400: train loss 0.00153\n",
      "iter_dt 9.02ms; iter 16500: train loss 0.00208\n",
      "iter_dt 8.98ms; iter 16600: train loss 0.00090\n",
      "iter_dt 9.00ms; iter 16700: train loss 0.00307\n",
      "iter_dt 8.99ms; iter 16800: train loss 0.00278\n",
      "iter_dt 8.99ms; iter 16900: train loss 0.00318\n",
      "iter_dt 0.26ms; iter 17000: train loss 0.00080\n",
      "iter_dt 9.00ms; iter 17100: train loss 0.00162\n",
      "iter_dt 10.01ms; iter 17200: train loss 0.00808\n",
      "iter_dt 10.11ms; iter 17300: train loss 0.00171\n",
      "iter_dt 9.00ms; iter 17400: train loss 0.00420\n",
      "iter_dt 9.02ms; iter 17500: train loss 0.00449\n",
      "iter_dt 8.00ms; iter 17600: train loss 0.00295\n",
      "iter_dt 8.01ms; iter 17700: train loss 0.00260\n",
      "iter_dt 10.11ms; iter 17800: train loss 0.00090\n",
      "iter_dt 9.02ms; iter 17900: train loss 0.00148\n",
      "iter_dt 9.95ms; iter 18000: train loss 0.00149\n",
      "iter_dt 0.00ms; iter 18100: train loss 0.00223\n",
      "iter_dt 8.33ms; iter 18200: train loss 0.00155\n",
      "iter_dt 9.00ms; iter 18300: train loss 0.00164\n",
      "iter_dt 9.32ms; iter 18400: train loss 0.18997\n",
      "iter_dt 9.02ms; iter 18500: train loss 0.00133\n",
      "iter_dt 7.98ms; iter 18600: train loss 0.00284\n",
      "iter_dt 10.00ms; iter 18700: train loss 0.00170\n",
      "iter_dt 10.01ms; iter 18800: train loss 0.00170\n",
      "iter_dt 9.00ms; iter 18900: train loss 0.00054\n",
      "iter_dt 9.91ms; iter 19000: train loss 0.00026\n",
      "iter_dt 8.89ms; iter 19100: train loss 0.00088\n",
      "iter_dt 8.90ms; iter 19200: train loss 0.00057\n",
      "iter_dt 9.00ms; iter 19300: train loss 0.00154\n",
      "iter_dt 9.02ms; iter 19400: train loss 0.00047\n",
      "iter_dt 9.40ms; iter 19500: train loss 0.00152\n",
      "iter_dt 8.00ms; iter 19600: train loss 0.00174\n",
      "iter_dt 8.00ms; iter 19700: train loss 0.00192\n",
      "iter_dt 8.92ms; iter 19800: train loss 0.00200\n",
      "iter_dt 9.00ms; iter 19900: train loss 0.00123\n",
      "iter_dt 9.02ms; iter 20000: train loss 0.00078\n",
      "iter_dt 9.00ms; iter 20100: train loss 0.00033\n",
      "iter_dt 9.00ms; iter 20200: train loss 0.00385\n",
      "iter_dt 10.41ms; iter 20300: train loss 0.00051\n",
      "iter_dt 9.28ms; iter 20400: train loss 0.00189\n",
      "iter_dt 9.00ms; iter 20500: train loss 0.00152\n",
      "iter_dt 8.00ms; iter 20600: train loss 0.00161\n",
      "iter_dt 9.96ms; iter 20700: train loss 0.00045\n",
      "iter_dt 9.02ms; iter 20800: train loss 0.00037\n",
      "iter_dt 10.00ms; iter 20900: train loss 0.00129\n",
      "iter_dt 8.90ms; iter 21000: train loss 0.00050\n",
      "iter_dt 9.89ms; iter 21100: train loss 0.00106\n",
      "iter_dt 9.00ms; iter 21200: train loss 0.00062\n",
      "iter_dt 9.01ms; iter 21300: train loss 0.00049\n",
      "iter_dt 9.01ms; iter 21400: train loss 0.00237\n",
      "iter_dt 10.09ms; iter 21500: train loss 0.00084\n",
      "iter_dt 8.90ms; iter 21600: train loss 0.00322\n",
      "iter_dt 8.95ms; iter 21700: train loss 0.00135\n",
      "iter_dt 8.92ms; iter 21800: train loss 0.00227\n",
      "iter_dt 9.51ms; iter 21900: train loss 0.00056\n",
      "iter_dt 9.01ms; iter 22000: train loss 0.00049\n",
      "iter_dt 9.71ms; iter 22100: train loss 0.00086\n",
      "iter_dt 9.01ms; iter 22200: train loss 0.00046\n",
      "iter_dt 9.00ms; iter 22300: train loss 0.00060\n",
      "iter_dt 11.53ms; iter 22400: train loss 0.00060\n",
      "iter_dt 9.00ms; iter 22500: train loss 0.00178\n",
      "iter_dt 9.12ms; iter 22600: train loss 0.00027\n",
      "iter_dt 9.01ms; iter 22700: train loss 0.00118\n",
      "iter_dt 10.00ms; iter 22800: train loss 0.00066\n",
      "iter_dt 8.99ms; iter 22900: train loss 0.00081\n",
      "iter_dt 9.00ms; iter 23000: train loss 0.00015\n",
      "iter_dt 9.01ms; iter 23100: train loss 0.00037\n",
      "iter_dt 9.11ms; iter 23200: train loss 0.00078\n",
      "iter_dt 10.01ms; iter 23300: train loss 0.00019\n",
      "iter_dt 9.98ms; iter 23400: train loss 0.00020\n",
      "iter_dt 9.10ms; iter 23500: train loss 0.00011\n",
      "iter_dt 10.00ms; iter 23600: train loss 0.00039\n",
      "iter_dt 9.00ms; iter 23700: train loss 0.00079\n",
      "iter_dt 9.11ms; iter 23800: train loss 0.00040\n",
      "iter_dt 9.52ms; iter 23900: train loss 0.00018\n",
      "iter_dt 9.02ms; iter 24000: train loss 0.00110\n",
      "iter_dt 8.97ms; iter 24100: train loss 0.00368\n",
      "iter_dt 9.99ms; iter 24200: train loss 0.00161\n",
      "iter_dt 10.15ms; iter 24300: train loss 0.00059\n",
      "iter_dt 9.00ms; iter 24400: train loss 0.00012\n",
      "iter_dt 9.14ms; iter 24500: train loss 0.00039\n",
      "iter_dt 9.43ms; iter 24600: train loss 0.00033\n",
      "iter_dt 9.89ms; iter 24700: train loss 0.00117\n",
      "iter_dt 10.00ms; iter 24800: train loss 0.00080\n",
      "iter_dt 9.42ms; iter 24900: train loss 0.00029\n",
      "iter_dt 9.02ms; iter 25000: train loss 0.00069\n",
      "iter_dt 9.30ms; iter 25100: train loss 0.00136\n",
      "iter_dt 10.01ms; iter 25200: train loss 0.00089\n",
      "iter_dt 16.00ms; iter 25300: train loss 0.00025\n",
      "iter_dt 9.19ms; iter 25400: train loss 0.00029\n",
      "iter_dt 10.03ms; iter 25500: train loss 0.00022\n",
      "iter_dt 0.00ms; iter 25600: train loss 0.00037\n",
      "iter_dt 10.02ms; iter 25700: train loss 0.00056\n",
      "iter_dt 9.00ms; iter 25800: train loss 0.00021\n",
      "iter_dt 10.00ms; iter 25900: train loss 0.00158\n",
      "iter_dt 8.95ms; iter 26000: train loss 0.00085\n",
      "iter_dt 8.92ms; iter 26100: train loss 0.00038\n",
      "iter_dt 10.00ms; iter 26200: train loss 0.00012\n",
      "iter_dt 10.02ms; iter 26300: train loss 0.21707\n",
      "iter_dt 11.01ms; iter 26400: train loss 0.00021\n",
      "iter_dt 10.02ms; iter 26500: train loss 0.00021\n",
      "iter_dt 9.00ms; iter 26600: train loss 0.00096\n",
      "iter_dt 9.93ms; iter 26700: train loss 0.00012\n",
      "iter_dt 8.97ms; iter 26800: train loss 0.00074\n",
      "iter_dt 10.00ms; iter 26900: train loss 0.00068\n",
      "iter_dt 9.94ms; iter 27000: train loss 0.00153\n",
      "iter_dt 9.98ms; iter 27100: train loss 0.00025\n",
      "iter_dt 10.99ms; iter 27200: train loss 0.00076\n",
      "iter_dt 9.99ms; iter 27300: train loss 0.00030\n",
      "iter_dt 10.00ms; iter 27400: train loss 0.00045\n",
      "iter_dt 10.00ms; iter 27500: train loss 0.00052\n",
      "iter_dt 9.46ms; iter 27600: train loss 0.00051\n",
      "iter_dt 9.33ms; iter 27700: train loss 0.00018\n",
      "iter_dt 8.94ms; iter 27800: train loss 0.00067\n",
      "iter_dt 9.89ms; iter 27900: train loss 0.00061\n",
      "iter_dt 11.06ms; iter 28000: train loss 0.00012\n",
      "iter_dt 10.98ms; iter 28100: train loss 0.00017\n",
      "iter_dt 0.00ms; iter 28200: train loss 0.00025\n",
      "iter_dt 10.10ms; iter 28300: train loss 0.00027\n",
      "iter_dt 9.99ms; iter 28400: train loss 0.00017\n",
      "iter_dt 9.04ms; iter 28500: train loss 0.00018\n",
      "iter_dt 10.00ms; iter 28600: train loss 0.00044\n",
      "iter_dt 12.98ms; iter 28700: train loss 0.00024\n",
      "iter_dt 10.00ms; iter 28800: train loss 0.00044\n",
      "iter_dt 9.02ms; iter 28900: train loss 0.00028\n",
      "iter_dt 10.13ms; iter 29000: train loss 0.00041\n",
      "iter_dt 12.03ms; iter 29100: train loss 0.00018\n",
      "iter_dt 10.00ms; iter 29200: train loss 0.00033\n",
      "iter_dt 10.01ms; iter 29300: train loss 0.00039\n",
      "iter_dt 8.98ms; iter 29400: train loss 0.00038\n",
      "iter_dt 8.91ms; iter 29500: train loss 0.00021\n",
      "iter_dt 11.98ms; iter 29600: train loss 0.00090\n",
      "iter_dt 9.00ms; iter 29700: train loss 0.00039\n",
      "iter_dt 9.05ms; iter 29800: train loss 0.00010\n",
      "iter_dt 9.74ms; iter 29900: train loss 0.00015\n",
      "iter_dt 9.46ms; iter 30000: train loss 0.00025\n",
      "iter_dt 9.17ms; iter 30100: train loss 0.00053\n",
      "iter_dt 9.72ms; iter 30200: train loss 0.00032\n",
      "iter_dt 11.00ms; iter 30300: train loss 0.00030\n",
      "iter_dt 9.01ms; iter 30400: train loss 0.00031\n",
      "iter_dt 9.00ms; iter 30500: train loss 0.00100\n",
      "iter_dt 10.00ms; iter 30600: train loss 0.00047\n",
      "iter_dt 9.98ms; iter 30700: train loss 0.00029\n",
      "iter_dt 10.47ms; iter 30800: train loss 0.00048\n",
      "iter_dt 9.50ms; iter 30900: train loss 0.00019\n",
      "iter_dt 9.98ms; iter 31000: train loss 0.00065\n",
      "iter_dt 9.98ms; iter 31100: train loss 0.00023\n",
      "iter_dt 10.06ms; iter 31200: train loss 0.00017\n",
      "iter_dt 10.00ms; iter 31300: train loss 0.00037\n",
      "iter_dt 10.00ms; iter 31400: train loss 0.00013\n",
      "iter_dt 9.00ms; iter 31500: train loss 0.00112\n",
      "iter_dt 9.51ms; iter 31600: train loss 0.00043\n",
      "iter_dt 10.00ms; iter 31700: train loss 0.00039\n",
      "iter_dt 9.00ms; iter 31800: train loss 0.00074\n",
      "iter_dt 10.02ms; iter 31900: train loss 0.00042\n",
      "iter_dt 10.01ms; iter 32000: train loss 0.00009\n",
      "iter_dt 11.00ms; iter 32100: train loss 0.00025\n",
      "iter_dt 11.08ms; iter 32200: train loss 0.00069\n",
      "iter_dt 9.00ms; iter 32300: train loss 0.00014\n",
      "iter_dt 10.00ms; iter 32400: train loss 0.00014\n",
      "iter_dt 10.00ms; iter 32500: train loss 0.00099\n",
      "iter_dt 9.01ms; iter 32600: train loss 0.00041\n",
      "iter_dt 9.01ms; iter 32700: train loss 0.00104\n",
      "iter_dt 20.42ms; iter 32800: train loss 0.00037\n",
      "iter_dt 9.53ms; iter 32900: train loss 0.00113\n",
      "iter_dt 9.99ms; iter 33000: train loss 0.00011\n",
      "iter_dt 10.10ms; iter 33100: train loss 0.00031\n",
      "iter_dt 10.09ms; iter 33200: train loss 0.00033\n",
      "iter_dt 8.91ms; iter 33300: train loss 0.00014\n",
      "iter_dt 10.00ms; iter 33400: train loss 0.00094\n",
      "iter_dt 8.98ms; iter 33500: train loss 0.00038\n",
      "iter_dt 9.43ms; iter 33600: train loss 0.00031\n",
      "iter_dt 9.02ms; iter 33700: train loss 0.00014\n",
      "iter_dt 9.97ms; iter 33800: train loss 0.00035\n",
      "iter_dt 9.82ms; iter 33900: train loss 0.00003\n",
      "iter_dt 9.04ms; iter 34000: train loss 0.00021\n",
      "iter_dt 9.09ms; iter 34100: train loss 0.00006\n",
      "iter_dt 10.00ms; iter 34200: train loss 0.00062\n",
      "iter_dt 9.99ms; iter 34300: train loss 0.00014\n",
      "iter_dt 11.00ms; iter 34400: train loss 0.00015\n",
      "iter_dt 9.00ms; iter 34500: train loss 0.00029\n",
      "iter_dt 9.46ms; iter 34600: train loss 0.00028\n",
      "iter_dt 11.00ms; iter 34700: train loss 0.00028\n",
      "iter_dt 9.91ms; iter 34800: train loss 0.00021\n",
      "iter_dt 10.32ms; iter 34900: train loss 0.00049\n",
      "iter_dt 9.00ms; iter 35000: train loss 0.00007\n",
      "iter_dt 10.97ms; iter 35100: train loss 0.00012\n",
      "iter_dt 9.79ms; iter 35200: train loss 0.00055\n",
      "iter_dt 10.99ms; iter 35300: train loss 0.00017\n",
      "iter_dt 10.03ms; iter 35400: train loss 0.00006\n",
      "iter_dt 9.08ms; iter 35500: train loss 0.00026\n",
      "iter_dt 9.01ms; iter 35600: train loss 0.00002\n",
      "iter_dt 10.02ms; iter 35700: train loss 0.00007\n",
      "iter_dt 11.55ms; iter 35800: train loss 0.00053\n",
      "iter_dt 10.13ms; iter 35900: train loss 0.00026\n",
      "iter_dt 10.51ms; iter 36000: train loss 0.00022\n",
      "iter_dt 10.00ms; iter 36100: train loss 0.00043\n",
      "iter_dt 9.09ms; iter 36200: train loss 0.00016\n",
      "iter_dt 10.02ms; iter 36300: train loss 0.00003\n",
      "iter_dt 10.67ms; iter 36400: train loss 0.00026\n",
      "iter_dt 11.00ms; iter 36500: train loss 0.00078\n",
      "iter_dt 9.99ms; iter 36600: train loss 0.00020\n",
      "iter_dt 22.09ms; iter 36700: train loss 0.00018\n",
      "iter_dt 15.82ms; iter 36800: train loss 0.00019\n",
      "iter_dt 10.99ms; iter 36900: train loss 0.00037\n",
      "iter_dt 10.00ms; iter 37000: train loss 0.00040\n",
      "iter_dt 9.00ms; iter 37100: train loss 0.00017\n",
      "iter_dt 10.00ms; iter 37200: train loss 0.00012\n",
      "iter_dt 10.00ms; iter 37300: train loss 0.00015\n",
      "iter_dt 10.00ms; iter 37400: train loss 0.00010\n",
      "iter_dt 5.06ms; iter 37500: train loss 0.00018\n",
      "iter_dt 11.01ms; iter 37600: train loss 0.00011\n",
      "iter_dt 8.98ms; iter 37700: train loss 0.00011\n",
      "iter_dt 9.00ms; iter 37800: train loss 0.00036\n",
      "iter_dt 9.02ms; iter 37900: train loss 0.00083\n",
      "iter_dt 10.00ms; iter 38000: train loss 0.00011\n",
      "iter_dt 8.78ms; iter 38100: train loss 0.00007\n",
      "iter_dt 9.53ms; iter 38200: train loss 0.00008\n",
      "iter_dt 10.00ms; iter 38300: train loss 0.00012\n",
      "iter_dt 8.98ms; iter 38400: train loss 0.00031\n",
      "iter_dt 11.01ms; iter 38500: train loss 0.00015\n",
      "iter_dt 11.08ms; iter 38600: train loss 0.00016\n",
      "iter_dt 10.01ms; iter 38700: train loss 0.00079\n",
      "iter_dt 15.04ms; iter 38800: train loss 0.00012\n",
      "iter_dt 9.00ms; iter 38900: train loss 0.00033\n",
      "iter_dt 10.00ms; iter 39000: train loss 0.00015\n",
      "iter_dt 10.99ms; iter 39100: train loss 0.00016\n",
      "iter_dt 9.91ms; iter 39200: train loss 0.00013\n",
      "iter_dt 10.00ms; iter 39300: train loss 0.00046\n",
      "iter_dt 10.00ms; iter 39400: train loss 0.00040\n",
      "iter_dt 10.75ms; iter 39500: train loss 0.00007\n",
      "iter_dt 10.00ms; iter 39600: train loss 0.00028\n",
      "iter_dt 9.69ms; iter 39700: train loss 0.00006\n",
      "iter_dt 11.00ms; iter 39800: train loss 0.00017\n",
      "iter_dt 8.99ms; iter 39900: train loss 0.00020\n",
      "iter_dt 10.00ms; iter 40000: train loss 0.00008\n",
      "iter_dt 10.00ms; iter 40100: train loss 0.00017\n",
      "iter_dt 8.99ms; iter 40200: train loss 0.00016\n",
      "iter_dt 10.98ms; iter 40300: train loss 0.00008\n",
      "iter_dt 10.71ms; iter 40400: train loss 0.00045\n",
      "iter_dt 9.73ms; iter 40500: train loss 0.23953\n",
      "iter_dt 9.93ms; iter 40600: train loss 0.00029\n",
      "iter_dt 16.25ms; iter 40700: train loss 0.00035\n",
      "iter_dt 9.45ms; iter 40800: train loss 0.00015\n",
      "iter_dt 10.00ms; iter 40900: train loss 0.00021\n",
      "iter_dt 9.99ms; iter 41000: train loss 0.00019\n",
      "iter_dt 10.02ms; iter 41100: train loss 0.00030\n",
      "iter_dt 12.51ms; iter 41200: train loss 0.00019\n",
      "iter_dt 11.01ms; iter 41300: train loss 0.00004\n",
      "iter_dt 9.74ms; iter 41400: train loss 0.00030\n",
      "iter_dt 9.96ms; iter 41500: train loss 0.00010\n",
      "iter_dt 10.02ms; iter 41600: train loss 0.00009\n",
      "iter_dt 10.50ms; iter 41700: train loss 0.00028\n",
      "iter_dt 10.01ms; iter 41800: train loss 0.00022\n",
      "iter_dt 10.11ms; iter 41900: train loss 0.00049\n",
      "iter_dt 10.02ms; iter 42000: train loss 0.00015\n",
      "iter_dt 9.99ms; iter 42100: train loss 0.00027\n",
      "iter_dt 10.01ms; iter 42200: train loss 0.00038\n",
      "iter_dt 10.00ms; iter 42300: train loss 0.00006\n",
      "iter_dt 10.00ms; iter 42400: train loss 0.00015\n",
      "iter_dt 10.11ms; iter 42500: train loss 0.00006\n",
      "iter_dt 9.98ms; iter 42600: train loss 0.00014\n",
      "iter_dt 11.00ms; iter 42700: train loss 0.00016\n",
      "iter_dt 10.05ms; iter 42800: train loss 0.00010\n",
      "iter_dt 9.44ms; iter 42900: train loss 0.00006\n",
      "iter_dt 9.75ms; iter 43000: train loss 0.00028\n",
      "iter_dt 9.98ms; iter 43100: train loss 0.00009\n",
      "iter_dt 9.02ms; iter 43200: train loss 0.00011\n",
      "iter_dt 11.01ms; iter 43300: train loss 0.00021\n",
      "iter_dt 10.00ms; iter 43400: train loss 0.00008\n",
      "iter_dt 9.06ms; iter 43500: train loss 0.00012\n",
      "iter_dt 10.10ms; iter 43600: train loss 0.00032\n",
      "iter_dt 10.51ms; iter 43700: train loss 0.00041\n",
      "iter_dt 12.03ms; iter 43800: train loss 0.00052\n",
      "iter_dt 9.98ms; iter 43900: train loss 0.00010\n",
      "iter_dt 9.01ms; iter 44000: train loss 0.00012\n",
      "iter_dt 9.76ms; iter 44100: train loss 0.00004\n",
      "iter_dt 9.97ms; iter 44200: train loss 0.00003\n",
      "iter_dt 9.93ms; iter 44300: train loss 0.00011\n",
      "iter_dt 9.00ms; iter 44400: train loss 0.00030\n",
      "iter_dt 9.71ms; iter 44500: train loss 0.00030\n",
      "iter_dt 9.74ms; iter 44600: train loss 0.00020\n",
      "iter_dt 9.85ms; iter 44700: train loss 0.00029\n",
      "iter_dt 9.71ms; iter 44800: train loss 0.00012\n",
      "iter_dt 10.01ms; iter 44900: train loss 0.00029\n",
      "iter_dt 10.92ms; iter 45000: train loss 0.00035\n",
      "iter_dt 9.11ms; iter 45100: train loss 0.00018\n",
      "iter_dt 9.01ms; iter 45200: train loss 0.00008\n",
      "iter_dt 10.10ms; iter 45300: train loss 0.00023\n",
      "iter_dt 12.00ms; iter 45400: train loss 0.00024\n",
      "iter_dt 9.02ms; iter 45500: train loss 0.00013\n",
      "iter_dt 10.01ms; iter 45600: train loss 0.00025\n",
      "iter_dt 10.01ms; iter 45700: train loss 0.00010\n",
      "iter_dt 10.02ms; iter 45800: train loss 0.00029\n",
      "iter_dt 8.98ms; iter 45900: train loss 0.23642\n",
      "iter_dt 10.00ms; iter 46000: train loss 0.00029\n",
      "iter_dt 1.48ms; iter 46100: train loss 0.00018\n",
      "iter_dt 11.95ms; iter 46200: train loss 0.00005\n",
      "iter_dt 9.00ms; iter 46300: train loss 0.00025\n",
      "iter_dt 10.02ms; iter 46400: train loss 0.00021\n",
      "iter_dt 10.00ms; iter 46500: train loss 0.00025\n",
      "iter_dt 10.14ms; iter 46600: train loss 0.00010\n",
      "iter_dt 9.98ms; iter 46700: train loss 0.00014\n",
      "iter_dt 11.00ms; iter 46800: train loss 0.00025\n",
      "iter_dt 9.00ms; iter 46900: train loss 0.00010\n",
      "iter_dt 10.07ms; iter 47000: train loss 0.00023\n",
      "iter_dt 9.74ms; iter 47100: train loss 0.24237\n",
      "iter_dt 10.11ms; iter 47200: train loss 0.00009\n",
      "iter_dt 9.90ms; iter 47300: train loss 0.00027\n",
      "iter_dt 12.00ms; iter 47400: train loss 0.00042\n",
      "iter_dt 11.03ms; iter 47500: train loss 0.00022\n",
      "iter_dt 9.01ms; iter 47600: train loss 0.00031\n",
      "iter_dt 10.00ms; iter 47700: train loss 0.00005\n",
      "iter_dt 10.10ms; iter 47800: train loss 0.00027\n",
      "iter_dt 11.78ms; iter 47900: train loss 0.00010\n",
      "iter_dt 8.91ms; iter 48000: train loss 0.00026\n",
      "iter_dt 9.04ms; iter 48100: train loss 0.00009\n",
      "iter_dt 9.88ms; iter 48200: train loss 0.00010\n",
      "iter_dt 9.00ms; iter 48300: train loss 0.00009\n",
      "iter_dt 12.09ms; iter 48400: train loss 0.00014\n",
      "iter_dt 13.00ms; iter 48500: train loss 0.00014\n",
      "iter_dt 9.84ms; iter 48600: train loss 0.00019\n",
      "iter_dt 17.00ms; iter 48700: train loss 0.00020\n",
      "iter_dt 11.01ms; iter 48800: train loss 0.00021\n",
      "iter_dt 11.00ms; iter 48900: train loss 0.00016\n",
      "iter_dt 13.53ms; iter 49000: train loss 0.00020\n",
      "iter_dt 10.08ms; iter 49100: train loss 0.00006\n",
      "iter_dt 10.00ms; iter 49200: train loss 0.00008\n",
      "iter_dt 10.02ms; iter 49300: train loss 0.00111\n",
      "iter_dt 10.93ms; iter 49400: train loss 0.00054\n",
      "iter_dt 10.58ms; iter 49500: train loss 0.00033\n",
      "iter_dt 8.92ms; iter 49600: train loss 0.00024\n",
      "iter_dt 9.95ms; iter 49700: train loss 0.00029\n",
      "iter_dt 10.00ms; iter 49800: train loss 0.00022\n",
      "iter_dt 10.00ms; iter 49900: train loss 0.00031\n",
      "iter_dt 11.10ms; iter 50000: train loss 0.00056\n",
      "iter_dt 11.00ms; iter 50100: train loss 0.00004\n",
      "iter_dt 10.12ms; iter 50200: train loss 0.00014\n",
      "iter_dt 9.59ms; iter 50300: train loss 0.00010\n",
      "iter_dt 10.01ms; iter 50400: train loss 0.00029\n",
      "iter_dt 10.00ms; iter 50500: train loss 0.00012\n",
      "iter_dt 13.03ms; iter 50600: train loss 0.00022\n",
      "iter_dt 9.53ms; iter 50700: train loss 0.00128\n",
      "iter_dt 8.95ms; iter 50800: train loss 0.00014\n",
      "iter_dt 10.01ms; iter 50900: train loss 0.00005\n",
      "iter_dt 23.11ms; iter 51000: train loss 0.00021\n",
      "iter_dt 13.00ms; iter 51100: train loss 0.00015\n",
      "iter_dt 10.91ms; iter 51200: train loss 0.00007\n",
      "iter_dt 10.06ms; iter 51300: train loss 0.00039\n",
      "iter_dt 15.73ms; iter 51400: train loss 0.00037\n",
      "iter_dt 0.00ms; iter 51500: train loss 0.00020\n",
      "iter_dt 3.10ms; iter 51600: train loss 0.00008\n",
      "iter_dt 9.95ms; iter 51700: train loss 0.00054\n",
      "iter_dt 9.01ms; iter 51800: train loss 0.00007\n",
      "iter_dt 9.90ms; iter 51900: train loss 0.00059\n",
      "iter_dt 10.11ms; iter 52000: train loss 0.00038\n",
      "iter_dt 9.90ms; iter 52100: train loss 0.00015\n",
      "iter_dt 9.58ms; iter 52200: train loss 0.05697\n",
      "iter_dt 9.98ms; iter 52300: train loss 0.00018\n",
      "iter_dt 10.15ms; iter 52400: train loss 0.00019\n",
      "iter_dt 9.98ms; iter 52500: train loss 0.00023\n",
      "iter_dt 12.00ms; iter 52600: train loss 0.00013\n",
      "iter_dt 9.99ms; iter 52700: train loss 0.00005\n",
      "iter_dt 9.05ms; iter 52800: train loss 0.00013\n",
      "iter_dt 10.01ms; iter 52900: train loss 0.00029\n",
      "iter_dt 9.22ms; iter 53000: train loss 0.00007\n",
      "iter_dt 8.90ms; iter 53100: train loss 0.00028\n",
      "iter_dt 8.96ms; iter 53200: train loss 0.00002\n",
      "iter_dt 9.88ms; iter 53300: train loss 0.00036\n",
      "iter_dt 12.00ms; iter 53400: train loss 0.00007\n",
      "iter_dt 10.02ms; iter 53500: train loss 0.00053\n",
      "iter_dt 10.00ms; iter 53600: train loss 0.00015\n",
      "iter_dt 10.01ms; iter 53700: train loss 0.00024\n",
      "iter_dt 10.98ms; iter 53800: train loss 0.00031\n",
      "iter_dt 17.10ms; iter 53900: train loss 0.00013\n",
      "iter_dt 14.52ms; iter 54000: train loss 0.00012\n",
      "iter_dt 14.10ms; iter 54100: train loss 0.00018\n",
      "iter_dt 18.98ms; iter 54200: train loss 0.00017\n",
      "iter_dt 10.01ms; iter 54300: train loss 0.00012\n",
      "iter_dt 10.00ms; iter 54400: train loss 0.00007\n",
      "iter_dt 10.13ms; iter 54500: train loss 0.00139\n",
      "iter_dt 14.98ms; iter 54600: train loss 0.00015\n",
      "iter_dt 13.00ms; iter 54700: train loss 0.00026\n",
      "iter_dt 12.00ms; iter 54800: train loss 0.00011\n",
      "iter_dt 30.00ms; iter 54900: train loss 0.00050\n",
      "iter_dt 15.66ms; iter 55000: train loss 0.00014\n",
      "iter_dt 36.90ms; iter 55100: train loss 0.00021\n",
      "iter_dt 21.12ms; iter 55200: train loss 0.00017\n",
      "iter_dt 23.05ms; iter 55300: train loss 0.00022\n",
      "iter_dt 12.04ms; iter 55400: train loss 0.00027\n",
      "iter_dt 10.60ms; iter 55500: train loss 0.00027\n",
      "iter_dt 10.61ms; iter 55600: train loss 0.00011\n",
      "iter_dt 36.14ms; iter 55700: train loss 0.00036\n",
      "iter_dt 28.34ms; iter 55800: train loss 0.00009\n",
      "iter_dt 15.09ms; iter 55900: train loss 0.00033\n",
      "iter_dt 30.40ms; iter 56000: train loss 0.00311\n",
      "iter_dt 34.06ms; iter 56100: train loss 0.00031\n",
      "iter_dt 14.63ms; iter 56200: train loss 0.00013\n",
      "iter_dt 12.63ms; iter 56300: train loss 0.00019\n",
      "iter_dt 12.72ms; iter 56400: train loss 0.00012\n",
      "iter_dt 26.42ms; iter 56500: train loss 0.00016\n",
      "iter_dt 49.69ms; iter 56600: train loss 0.00005\n",
      "iter_dt 39.75ms; iter 56700: train loss 0.00009\n",
      "iter_dt 22.76ms; iter 56800: train loss 0.00016\n",
      "iter_dt 59.91ms; iter 56900: train loss 0.00019\n",
      "iter_dt 16.89ms; iter 57000: train loss 0.00003\n",
      "iter_dt 11.71ms; iter 57100: train loss 0.00012\n",
      "iter_dt 20.59ms; iter 57200: train loss 0.00004\n",
      "iter_dt 31.84ms; iter 57300: train loss 0.00005\n",
      "iter_dt 30.96ms; iter 57400: train loss 0.00008\n",
      "iter_dt 19.30ms; iter 57500: train loss 0.00026\n",
      "iter_dt 26.33ms; iter 57600: train loss 0.00017\n",
      "iter_dt 22.22ms; iter 57700: train loss 0.00005\n",
      "iter_dt 30.24ms; iter 57800: train loss 0.00020\n",
      "iter_dt 30.18ms; iter 57900: train loss 0.00001\n",
      "iter_dt 19.00ms; iter 58000: train loss 0.00010\n",
      "iter_dt 20.21ms; iter 58100: train loss 0.00005\n",
      "iter_dt 21.65ms; iter 58200: train loss 0.00020\n",
      "iter_dt 25.39ms; iter 58300: train loss 0.00052\n",
      "iter_dt 21.14ms; iter 58400: train loss 0.00013\n",
      "iter_dt 26.44ms; iter 58500: train loss 0.00033\n",
      "iter_dt 15.33ms; iter 58600: train loss 0.00039\n",
      "iter_dt 17.82ms; iter 58700: train loss 0.00030\n",
      "iter_dt 12.68ms; iter 58800: train loss 0.00037\n",
      "iter_dt 18.09ms; iter 58900: train loss 0.00064\n",
      "iter_dt 19.44ms; iter 59000: train loss 0.00007\n",
      "iter_dt 12.10ms; iter 59100: train loss 0.00030\n",
      "iter_dt 15.79ms; iter 59200: train loss 0.00041\n",
      "iter_dt 14.23ms; iter 59300: train loss 0.00007\n",
      "iter_dt 22.28ms; iter 59400: train loss 0.00006\n",
      "iter_dt 20.22ms; iter 59500: train loss 0.00054\n",
      "iter_dt 13.61ms; iter 59600: train loss 0.00013\n",
      "iter_dt 19.58ms; iter 59700: train loss 0.00016\n",
      "iter_dt 17.01ms; iter 59800: train loss 0.00009\n",
      "iter_dt 13.39ms; iter 59900: train loss 0.00016\n",
      "iter_dt 15.66ms; iter 60000: train loss 0.00007\n",
      "iter_dt 12.72ms; iter 60100: train loss 0.00017\n",
      "iter_dt 16.56ms; iter 60200: train loss 0.00287\n",
      "iter_dt 11.90ms; iter 60300: train loss 0.00005\n",
      "iter_dt 13.73ms; iter 60400: train loss 0.00006\n",
      "iter_dt 14.73ms; iter 60500: train loss 0.00024\n",
      "iter_dt 14.73ms; iter 60600: train loss 0.00011\n",
      "iter_dt 12.67ms; iter 60700: train loss 0.00008\n",
      "iter_dt 14.16ms; iter 60800: train loss 0.00011\n",
      "iter_dt 14.85ms; iter 60900: train loss 0.00004\n",
      "iter_dt 18.72ms; iter 61000: train loss 0.00006\n",
      "iter_dt 18.45ms; iter 61100: train loss 0.00005\n",
      "iter_dt 15.23ms; iter 61200: train loss 0.00012\n",
      "iter_dt 20.13ms; iter 61300: train loss 0.00012\n",
      "iter_dt 14.23ms; iter 61400: train loss 0.00024\n",
      "iter_dt 17.11ms; iter 61500: train loss 0.00024\n",
      "iter_dt 14.50ms; iter 61600: train loss 0.00008\n",
      "iter_dt 19.58ms; iter 61700: train loss 0.00014\n",
      "iter_dt 20.23ms; iter 61800: train loss 0.00001\n",
      "iter_dt 13.31ms; iter 61900: train loss 0.00008\n",
      "iter_dt 16.47ms; iter 62000: train loss 0.00014\n",
      "iter_dt 18.15ms; iter 62100: train loss 0.00023\n",
      "iter_dt 13.85ms; iter 62200: train loss 0.00015\n",
      "iter_dt 13.44ms; iter 62300: train loss 0.00017\n",
      "iter_dt 17.23ms; iter 62400: train loss 0.00010\n",
      "iter_dt 12.73ms; iter 62500: train loss 0.00006\n",
      "iter_dt 26.27ms; iter 62600: train loss 0.00014\n",
      "iter_dt 15.53ms; iter 62700: train loss 0.00041\n",
      "iter_dt 14.32ms; iter 62800: train loss 0.00003\n",
      "iter_dt 16.00ms; iter 62900: train loss 0.00027\n",
      "iter_dt 14.52ms; iter 63000: train loss 0.00002\n",
      "iter_dt 14.58ms; iter 63100: train loss 0.00013\n",
      "iter_dt 12.91ms; iter 63200: train loss 0.24644\n",
      "iter_dt 13.95ms; iter 63300: train loss 0.00009\n",
      "iter_dt 13.93ms; iter 63400: train loss 0.00006\n",
      "iter_dt 12.48ms; iter 63500: train loss 0.00020\n",
      "iter_dt 17.20ms; iter 63600: train loss 0.00044\n",
      "iter_dt 14.40ms; iter 63700: train loss 0.00017\n",
      "iter_dt 13.82ms; iter 63800: train loss 0.00070\n",
      "iter_dt 22.40ms; iter 63900: train loss 0.00009\n",
      "iter_dt 12.36ms; iter 64000: train loss 0.00022\n",
      "iter_dt 19.55ms; iter 64100: train loss 0.00004\n",
      "iter_dt 18.25ms; iter 64200: train loss 0.00003\n",
      "iter_dt 12.57ms; iter 64300: train loss 0.00154\n",
      "iter_dt 14.62ms; iter 64400: train loss 0.00016\n",
      "iter_dt 14.17ms; iter 64500: train loss 0.00007\n",
      "iter_dt 13.72ms; iter 64600: train loss 0.00007\n",
      "iter_dt 17.09ms; iter 64700: train loss 0.00004\n",
      "iter_dt 16.18ms; iter 64800: train loss 0.00011\n",
      "iter_dt 15.17ms; iter 64900: train loss 0.00008\n",
      "iter_dt 15.03ms; iter 65000: train loss 0.00025\n",
      "iter_dt 14.65ms; iter 65100: train loss 0.00003\n",
      "iter_dt 12.67ms; iter 65200: train loss 0.00015\n",
      "iter_dt 18.31ms; iter 65300: train loss 0.00014\n",
      "iter_dt 13.23ms; iter 65400: train loss 0.00012\n",
      "iter_dt 11.66ms; iter 65500: train loss 0.00008\n",
      "iter_dt 24.87ms; iter 65600: train loss 0.00003\n",
      "iter_dt 13.44ms; iter 65700: train loss 0.00017\n",
      "iter_dt 12.31ms; iter 65800: train loss 0.00009\n",
      "iter_dt 12.80ms; iter 65900: train loss 0.00015\n",
      "iter_dt 19.03ms; iter 66000: train loss 0.00011\n",
      "iter_dt 12.41ms; iter 66100: train loss 0.00009\n",
      "iter_dt 11.71ms; iter 66200: train loss 0.00007\n",
      "iter_dt 18.18ms; iter 66300: train loss 0.00009\n",
      "iter_dt 15.71ms; iter 66400: train loss 0.00019\n",
      "iter_dt 19.29ms; iter 66500: train loss 0.00028\n",
      "iter_dt 13.70ms; iter 66600: train loss 0.00006\n",
      "iter_dt 18.53ms; iter 66700: train loss 0.00010\n",
      "iter_dt 12.62ms; iter 66800: train loss 0.00003\n",
      "iter_dt 12.72ms; iter 66900: train loss 0.00020\n",
      "iter_dt 26.71ms; iter 67000: train loss 0.00015\n",
      "iter_dt 15.40ms; iter 67100: train loss 0.00045\n",
      "iter_dt 18.01ms; iter 67200: train loss 0.24790\n",
      "iter_dt 15.25ms; iter 67300: train loss 0.00005\n",
      "iter_dt 13.71ms; iter 67400: train loss 0.00009\n",
      "iter_dt 13.57ms; iter 67500: train loss 0.00019\n",
      "iter_dt 16.46ms; iter 67600: train loss 0.00043\n",
      "iter_dt 27.01ms; iter 67700: train loss 0.00005\n",
      "iter_dt 11.67ms; iter 67800: train loss 0.00007\n",
      "iter_dt 21.04ms; iter 67900: train loss 0.00006\n",
      "iter_dt 12.64ms; iter 68000: train loss 0.00034\n",
      "iter_dt 16.39ms; iter 68100: train loss 0.00014\n",
      "iter_dt 14.29ms; iter 68200: train loss 0.00023\n",
      "iter_dt 17.65ms; iter 68300: train loss 0.00014\n",
      "iter_dt 12.42ms; iter 68400: train loss 0.00013\n",
      "iter_dt 12.85ms; iter 68500: train loss 0.00021\n",
      "iter_dt 23.23ms; iter 68600: train loss 0.00004\n",
      "iter_dt 11.71ms; iter 68700: train loss 0.00007\n",
      "iter_dt 12.38ms; iter 68800: train loss 0.00143\n",
      "iter_dt 28.15ms; iter 68900: train loss 0.00007\n",
      "iter_dt 17.46ms; iter 69000: train loss 0.00018\n",
      "iter_dt 13.81ms; iter 69100: train loss 0.00005\n",
      "iter_dt 11.70ms; iter 69200: train loss 0.00011\n",
      "iter_dt 12.74ms; iter 69300: train loss 0.00009\n",
      "iter_dt 16.42ms; iter 69400: train loss 0.00009\n",
      "iter_dt 14.59ms; iter 69500: train loss 0.00006\n",
      "iter_dt 11.62ms; iter 69600: train loss 0.00007\n",
      "iter_dt 11.66ms; iter 69700: train loss 0.00004\n",
      "iter_dt 14.51ms; iter 69800: train loss 0.00020\n",
      "iter_dt 16.13ms; iter 69900: train loss 0.00009\n",
      "iter_dt 11.71ms; iter 70000: train loss 0.00010\n",
      "iter_dt 11.76ms; iter 70100: train loss 0.00006\n",
      "iter_dt 11.67ms; iter 70200: train loss 0.00008\n",
      "iter_dt 16.74ms; iter 70300: train loss 0.00010\n",
      "iter_dt 17.31ms; iter 70400: train loss 0.00021\n",
      "iter_dt 14.85ms; iter 70500: train loss 0.00010\n",
      "iter_dt 12.69ms; iter 70600: train loss 0.00006\n",
      "iter_dt 13.34ms; iter 70700: train loss 0.00009\n",
      "iter_dt 14.22ms; iter 70800: train loss 0.00011\n",
      "iter_dt 13.43ms; iter 70900: train loss 0.00007\n",
      "iter_dt 16.22ms; iter 71000: train loss 0.00010\n",
      "iter_dt 13.37ms; iter 71100: train loss 0.00008\n",
      "iter_dt 16.39ms; iter 71200: train loss 0.00014\n",
      "iter_dt 12.38ms; iter 71300: train loss 0.00014\n",
      "iter_dt 13.39ms; iter 71400: train loss 0.00016\n",
      "iter_dt 13.72ms; iter 71500: train loss 0.00014\n",
      "iter_dt 13.68ms; iter 71600: train loss 0.00004\n",
      "iter_dt 12.68ms; iter 71700: train loss 0.18184\n",
      "iter_dt 12.71ms; iter 71800: train loss 0.00009\n",
      "iter_dt 12.79ms; iter 71900: train loss 0.00007\n",
      "iter_dt 17.28ms; iter 72000: train loss 0.00007\n",
      "iter_dt 12.73ms; iter 72100: train loss 0.00014\n",
      "iter_dt 18.32ms; iter 72200: train loss 0.00005\n",
      "iter_dt 19.92ms; iter 72300: train loss 0.00017\n",
      "iter_dt 12.40ms; iter 72400: train loss 0.00005\n",
      "iter_dt 12.59ms; iter 72500: train loss 0.00017\n",
      "iter_dt 16.83ms; iter 72600: train loss 0.00007\n",
      "iter_dt 16.78ms; iter 72700: train loss 0.00006\n",
      "iter_dt 16.72ms; iter 72800: train loss 0.00013\n",
      "iter_dt 19.29ms; iter 72900: train loss 0.00008\n",
      "iter_dt 11.77ms; iter 73000: train loss 0.00014\n",
      "iter_dt 17.92ms; iter 73100: train loss 0.00006\n",
      "iter_dt 12.71ms; iter 73200: train loss 0.00025\n",
      "iter_dt 18.44ms; iter 73300: train loss 0.00017\n",
      "iter_dt 16.43ms; iter 73400: train loss 0.00022\n",
      "iter_dt 12.54ms; iter 73500: train loss 0.00006\n",
      "iter_dt 11.74ms; iter 73600: train loss 0.00011\n",
      "iter_dt 11.63ms; iter 73700: train loss 0.00033\n",
      "iter_dt 14.93ms; iter 73800: train loss 0.00012\n",
      "iter_dt 17.96ms; iter 73900: train loss 0.00007\n",
      "iter_dt 15.76ms; iter 74000: train loss 0.00016\n",
      "iter_dt 12.22ms; iter 74100: train loss 0.00005\n",
      "iter_dt 12.53ms; iter 74200: train loss 0.00005\n",
      "iter_dt 12.45ms; iter 74300: train loss 0.00002\n",
      "iter_dt 12.54ms; iter 74400: train loss 0.00023\n",
      "iter_dt 11.51ms; iter 74500: train loss 0.00005\n",
      "iter_dt 12.51ms; iter 74600: train loss 0.00016\n",
      "iter_dt 12.51ms; iter 74700: train loss 0.00036\n",
      "iter_dt 12.82ms; iter 74800: train loss 0.00007\n",
      "iter_dt 13.55ms; iter 74900: train loss 0.00008\n",
      "iter_dt 12.50ms; iter 75000: train loss 0.00002\n",
      "iter_dt 11.79ms; iter 75100: train loss 0.00098\n",
      "iter_dt 12.51ms; iter 75200: train loss 0.00009\n",
      "iter_dt 12.53ms; iter 75300: train loss 0.00044\n",
      "iter_dt 12.55ms; iter 75400: train loss 0.00011\n",
      "iter_dt 18.03ms; iter 75500: train loss 0.00010\n",
      "iter_dt 13.51ms; iter 75600: train loss 0.00012\n",
      "iter_dt 16.50ms; iter 75700: train loss 0.00030\n",
      "iter_dt 15.53ms; iter 75800: train loss 0.00004\n",
      "iter_dt 12.53ms; iter 75900: train loss 0.00005\n",
      "iter_dt 12.55ms; iter 76000: train loss 0.00026\n",
      "iter_dt 13.55ms; iter 76100: train loss 0.00004\n",
      "iter_dt 16.00ms; iter 76200: train loss 0.00038\n",
      "iter_dt 13.16ms; iter 76300: train loss 0.00009\n",
      "iter_dt 21.04ms; iter 76400: train loss 0.00010\n",
      "iter_dt 13.54ms; iter 76500: train loss 0.00481\n",
      "iter_dt 18.72ms; iter 76600: train loss 0.00024\n",
      "iter_dt 18.32ms; iter 76700: train loss 0.00007\n",
      "iter_dt 12.72ms; iter 76800: train loss 0.00003\n",
      "iter_dt 13.56ms; iter 76900: train loss 0.00018\n",
      "iter_dt 18.69ms; iter 77000: train loss 0.00007\n",
      "iter_dt 18.37ms; iter 77100: train loss 0.00002\n",
      "iter_dt 13.71ms; iter 77200: train loss 0.00006\n",
      "iter_dt 13.56ms; iter 77300: train loss 0.00008\n",
      "iter_dt 12.99ms; iter 77400: train loss 0.00006\n",
      "iter_dt 14.03ms; iter 77500: train loss 0.00024\n",
      "iter_dt 13.60ms; iter 77600: train loss 0.00011\n",
      "iter_dt 14.07ms; iter 77700: train loss 0.00004\n",
      "iter_dt 17.12ms; iter 77800: train loss 0.00008\n",
      "iter_dt 13.07ms; iter 77900: train loss 0.00030\n",
      "iter_dt 19.04ms; iter 78000: train loss 0.00229\n",
      "iter_dt 23.08ms; iter 78100: train loss 0.00006\n",
      "iter_dt 13.49ms; iter 78200: train loss 0.00021\n",
      "iter_dt 12.43ms; iter 78300: train loss 0.00003\n",
      "iter_dt 15.54ms; iter 78400: train loss 0.00019\n",
      "iter_dt 13.02ms; iter 78500: train loss 0.00010\n",
      "iter_dt 14.03ms; iter 78600: train loss 0.00013\n",
      "iter_dt 11.47ms; iter 78700: train loss 0.00005\n",
      "iter_dt 14.55ms; iter 78800: train loss 0.00006\n",
      "iter_dt 21.59ms; iter 78900: train loss 0.00003\n",
      "iter_dt 23.24ms; iter 79000: train loss 0.00007\n",
      "iter_dt 19.99ms; iter 79100: train loss 0.00011\n",
      "iter_dt 12.72ms; iter 79200: train loss 0.00005\n",
      "iter_dt 13.57ms; iter 79300: train loss 0.00004\n",
      "iter_dt 12.07ms; iter 79400: train loss 0.00016\n",
      "iter_dt 14.65ms; iter 79500: train loss 0.00005\n",
      "iter_dt 12.42ms; iter 79600: train loss 0.00011\n",
      "iter_dt 12.17ms; iter 79700: train loss 0.00014\n",
      "iter_dt 12.55ms; iter 79800: train loss 0.00004\n",
      "iter_dt 12.08ms; iter 79900: train loss 0.00006\n",
      "iter_dt 16.50ms; iter 80000: train loss 0.00007\n",
      "iter_dt 16.30ms; iter 80100: train loss 0.00012\n",
      "iter_dt 12.13ms; iter 80200: train loss 0.00009\n",
      "iter_dt 12.52ms; iter 80300: train loss 0.00005\n",
      "iter_dt 12.53ms; iter 80400: train loss 0.00009\n",
      "iter_dt 12.54ms; iter 80500: train loss 0.00003\n",
      "iter_dt 14.28ms; iter 80600: train loss 0.00016\n",
      "iter_dt 11.55ms; iter 80700: train loss 0.00007\n",
      "iter_dt 11.80ms; iter 80800: train loss 0.00005\n",
      "iter_dt 28.51ms; iter 80900: train loss 0.00008\n",
      "iter_dt 39.11ms; iter 81000: train loss 0.00005\n",
      "iter_dt 16.06ms; iter 81100: train loss 0.00005\n",
      "iter_dt 20.93ms; iter 81200: train loss 0.00020\n",
      "iter_dt 14.00ms; iter 81300: train loss 0.00006\n",
      "iter_dt 15.09ms; iter 81400: train loss 0.00010\n",
      "iter_dt 14.55ms; iter 81500: train loss 0.00009\n",
      "iter_dt 17.09ms; iter 81600: train loss 0.00008\n",
      "iter_dt 12.63ms; iter 81700: train loss 0.00003\n",
      "iter_dt 12.50ms; iter 81800: train loss 0.00012\n",
      "iter_dt 12.85ms; iter 81900: train loss 0.00004\n",
      "iter_dt 12.71ms; iter 82000: train loss 0.00003\n",
      "iter_dt 23.48ms; iter 82100: train loss 0.00011\n",
      "iter_dt 30.93ms; iter 82200: train loss 0.00014\n",
      "iter_dt 41.79ms; iter 82300: train loss 0.00019\n",
      "iter_dt 29.77ms; iter 82400: train loss 0.00018\n",
      "iter_dt 24.99ms; iter 82500: train loss 0.00008\n",
      "iter_dt 29.69ms; iter 82600: train loss 0.00009\n",
      "iter_dt 30.23ms; iter 82700: train loss 0.00002\n",
      "iter_dt 18.69ms; iter 82800: train loss 0.00025\n",
      "iter_dt 47.20ms; iter 82900: train loss 0.00027\n",
      "iter_dt 44.06ms; iter 83000: train loss 0.00016\n",
      "iter_dt 66.77ms; iter 83100: train loss 0.00008\n",
      "iter_dt 27.91ms; iter 83200: train loss 0.00002\n",
      "iter_dt 58.11ms; iter 83300: train loss 0.00007\n",
      "iter_dt 33.54ms; iter 83400: train loss 0.00012\n",
      "iter_dt 27.21ms; iter 83500: train loss 0.00002\n",
      "iter_dt 37.32ms; iter 83600: train loss 0.00003\n",
      "iter_dt 49.58ms; iter 83700: train loss 0.00004\n",
      "iter_dt 79.60ms; iter 83800: train loss 0.00005\n",
      "iter_dt 13.52ms; iter 83900: train loss 0.00006\n",
      "iter_dt 21.11ms; iter 84000: train loss 0.00008\n",
      "iter_dt 12.74ms; iter 84100: train loss 0.00009\n",
      "iter_dt 44.45ms; iter 84200: train loss 0.00004\n",
      "iter_dt 40.24ms; iter 84300: train loss 0.00005\n",
      "iter_dt 26.84ms; iter 84400: train loss 0.00003\n",
      "iter_dt 43.04ms; iter 84500: train loss 0.00001\n",
      "iter_dt 47.55ms; iter 84600: train loss 0.00005\n",
      "iter_dt 13.43ms; iter 84700: train loss 0.00021\n",
      "iter_dt 12.76ms; iter 84800: train loss 0.00007\n",
      "iter_dt 13.51ms; iter 84900: train loss 0.00006\n",
      "iter_dt 12.96ms; iter 85000: train loss 0.00015\n",
      "iter_dt 12.66ms; iter 85100: train loss 0.00007\n",
      "iter_dt 10.71ms; iter 85200: train loss 0.00008\n",
      "iter_dt 12.66ms; iter 85300: train loss 0.00003\n",
      "iter_dt 12.65ms; iter 85400: train loss 0.00028\n",
      "iter_dt 16.51ms; iter 85500: train loss 0.00018\n",
      "iter_dt 48.04ms; iter 85600: train loss 0.00060\n",
      "iter_dt 28.53ms; iter 85700: train loss 0.00003\n",
      "iter_dt 42.62ms; iter 85800: train loss 0.00003\n",
      "iter_dt 58.31ms; iter 85900: train loss 0.00005\n",
      "iter_dt 44.57ms; iter 86000: train loss 0.00006\n",
      "iter_dt 31.85ms; iter 86100: train loss 0.00013\n",
      "iter_dt 64.55ms; iter 86200: train loss 0.00010\n",
      "iter_dt 40.93ms; iter 86300: train loss 0.00007\n",
      "iter_dt 37.46ms; iter 86400: train loss 0.00004\n",
      "iter_dt 48.83ms; iter 86500: train loss 0.00037\n",
      "iter_dt 17.36ms; iter 86600: train loss 0.00016\n",
      "iter_dt 12.49ms; iter 86700: train loss 0.00004\n",
      "iter_dt 13.57ms; iter 86800: train loss 0.00009\n",
      "iter_dt 12.76ms; iter 86900: train loss 0.00028\n",
      "iter_dt 16.51ms; iter 87000: train loss 0.00019\n",
      "iter_dt 15.49ms; iter 87100: train loss 0.00010\n",
      "iter_dt 12.80ms; iter 87200: train loss 0.00009\n",
      "iter_dt 16.98ms; iter 87300: train loss 0.00002\n",
      "iter_dt 12.52ms; iter 87400: train loss 0.00005\n",
      "iter_dt 42.92ms; iter 87500: train loss 0.00014\n",
      "iter_dt 14.37ms; iter 87600: train loss 0.00008\n",
      "iter_dt 15.38ms; iter 87700: train loss 0.00002\n",
      "iter_dt 18.40ms; iter 87800: train loss 0.00012\n",
      "iter_dt 23.28ms; iter 87900: train loss 0.00010\n",
      "iter_dt 13.77ms; iter 88000: train loss 0.00004\n",
      "iter_dt 23.98ms; iter 88100: train loss 0.00004\n",
      "iter_dt 21.56ms; iter 88200: train loss 0.00004\n",
      "iter_dt 19.99ms; iter 88300: train loss 0.00014\n",
      "iter_dt 13.86ms; iter 88400: train loss 0.00013\n",
      "iter_dt 18.28ms; iter 88500: train loss 0.00004\n",
      "iter_dt 13.56ms; iter 88600: train loss 0.00016\n",
      "iter_dt 15.73ms; iter 88700: train loss 0.00003\n",
      "iter_dt 12.71ms; iter 88800: train loss 0.00019\n",
      "iter_dt 11.75ms; iter 88900: train loss 0.00015\n",
      "iter_dt 15.30ms; iter 89000: train loss 0.00006\n",
      "iter_dt 20.49ms; iter 89100: train loss 0.00008\n",
      "iter_dt 17.28ms; iter 89200: train loss 0.00002\n",
      "iter_dt 26.25ms; iter 89300: train loss 0.00007\n",
      "iter_dt 31.30ms; iter 89400: train loss 0.00007\n",
      "iter_dt 17.45ms; iter 89500: train loss 0.00003\n",
      "iter_dt 17.43ms; iter 89600: train loss 0.00013\n",
      "iter_dt 23.25ms; iter 89700: train loss 0.00022\n",
      "iter_dt 38.29ms; iter 89800: train loss 0.00007\n",
      "iter_dt 13.36ms; iter 89900: train loss 0.00003\n",
      "iter_dt 72.58ms; iter 90000: train loss 0.00010\n",
      "iter_dt 20.26ms; iter 90100: train loss 0.00016\n",
      "iter_dt 25.00ms; iter 90200: train loss 0.00009\n",
      "iter_dt 25.33ms; iter 90300: train loss 0.00006\n",
      "iter_dt 32.34ms; iter 90400: train loss 0.00014\n",
      "iter_dt 16.66ms; iter 90500: train loss 0.00008\n",
      "iter_dt 11.72ms; iter 90600: train loss 0.00014\n",
      "iter_dt 13.72ms; iter 90700: train loss 0.00017\n",
      "iter_dt 13.74ms; iter 90800: train loss 0.00002\n",
      "iter_dt 19.37ms; iter 90900: train loss 0.00006\n",
      "iter_dt 19.22ms; iter 91000: train loss 0.00009\n",
      "iter_dt 20.79ms; iter 91100: train loss 0.00004\n",
      "iter_dt 30.59ms; iter 91200: train loss 0.00004\n",
      "iter_dt 12.70ms; iter 91300: train loss 0.00013\n",
      "iter_dt 28.52ms; iter 91400: train loss 0.00001\n",
      "iter_dt 11.66ms; iter 91500: train loss 0.00009\n",
      "iter_dt 12.62ms; iter 91600: train loss 0.00015\n",
      "iter_dt 24.21ms; iter 91700: train loss 0.00012\n",
      "iter_dt 25.68ms; iter 91800: train loss 0.00004\n",
      "iter_dt 21.21ms; iter 91900: train loss 0.00008\n",
      "iter_dt 14.52ms; iter 92000: train loss 0.00032\n",
      "iter_dt 12.22ms; iter 92100: train loss 0.00009\n",
      "iter_dt 12.52ms; iter 92200: train loss 0.00014\n",
      "iter_dt 12.61ms; iter 92300: train loss 0.00007\n",
      "iter_dt 29.78ms; iter 92400: train loss 0.00007\n",
      "iter_dt 31.69ms; iter 92500: train loss 0.00018\n",
      "iter_dt 28.62ms; iter 92600: train loss 0.00026\n",
      "iter_dt 44.22ms; iter 92700: train loss 0.00027\n",
      "iter_dt 36.72ms; iter 92800: train loss 0.00012\n",
      "iter_dt 32.62ms; iter 92900: train loss 0.00004\n",
      "iter_dt 40.72ms; iter 93000: train loss 0.00004\n",
      "iter_dt 13.69ms; iter 93100: train loss 0.00006\n",
      "iter_dt 41.18ms; iter 93200: train loss 0.00002\n",
      "iter_dt 26.76ms; iter 93300: train loss 0.00006\n",
      "iter_dt 62.93ms; iter 93400: train loss 0.00004\n",
      "iter_dt 76.10ms; iter 93500: train loss 0.00008\n",
      "iter_dt 32.80ms; iter 93600: train loss 0.00019\n",
      "iter_dt 58.54ms; iter 93700: train loss 0.00005\n",
      "iter_dt 30.19ms; iter 93800: train loss 0.00008\n",
      "iter_dt 36.03ms; iter 93900: train loss 0.00011\n",
      "iter_dt 25.83ms; iter 94000: train loss 0.00009\n",
      "iter_dt 40.80ms; iter 94100: train loss 0.00003\n",
      "iter_dt 51.70ms; iter 94200: train loss 0.00007\n",
      "iter_dt 41.45ms; iter 94300: train loss 0.00002\n",
      "iter_dt 49.82ms; iter 94400: train loss 0.00068\n",
      "iter_dt 52.17ms; iter 94500: train loss 0.00017\n",
      "iter_dt 28.51ms; iter 94600: train loss 0.00003\n",
      "iter_dt 35.96ms; iter 94700: train loss 0.00005\n",
      "iter_dt 38.06ms; iter 94800: train loss 0.00002\n",
      "iter_dt 65.37ms; iter 94900: train loss 0.00003\n",
      "iter_dt 37.69ms; iter 95000: train loss 0.00006\n",
      "iter_dt 37.30ms; iter 95100: train loss 0.00012\n",
      "iter_dt 19.25ms; iter 95200: train loss 0.00004\n",
      "iter_dt 12.55ms; iter 95300: train loss 0.00005\n",
      "iter_dt 55.96ms; iter 95400: train loss 0.00003\n",
      "iter_dt 61.74ms; iter 95500: train loss 0.00008\n",
      "iter_dt 41.69ms; iter 95600: train loss 0.00005\n",
      "iter_dt 39.32ms; iter 95700: train loss 0.00018\n",
      "iter_dt 28.56ms; iter 95800: train loss 0.00002\n",
      "iter_dt 38.76ms; iter 95900: train loss 0.00001\n",
      "iter_dt 30.82ms; iter 96000: train loss 0.00011\n",
      "iter_dt 40.16ms; iter 96100: train loss 0.00004\n",
      "iter_dt 30.63ms; iter 96200: train loss 0.00022\n",
      "iter_dt 28.82ms; iter 96300: train loss 0.00004\n",
      "iter_dt 36.77ms; iter 96400: train loss 0.00006\n",
      "iter_dt 54.26ms; iter 96500: train loss 0.00010\n",
      "iter_dt 28.83ms; iter 96600: train loss 0.00013\n",
      "iter_dt 26.95ms; iter 96700: train loss 0.00009\n",
      "iter_dt 17.23ms; iter 96800: train loss 0.00020\n",
      "iter_dt 11.60ms; iter 96900: train loss 0.00002\n",
      "iter_dt 12.63ms; iter 97000: train loss 0.00002\n",
      "iter_dt 12.92ms; iter 97100: train loss 0.00002\n",
      "iter_dt 13.25ms; iter 97200: train loss 0.00003\n",
      "iter_dt 28.08ms; iter 97300: train loss 0.00011\n",
      "iter_dt 18.66ms; iter 97400: train loss 0.00002\n",
      "iter_dt 15.27ms; iter 97500: train loss 0.00002\n",
      "iter_dt 23.72ms; iter 97600: train loss 0.00007\n",
      "iter_dt 29.50ms; iter 97700: train loss 0.00010\n",
      "iter_dt 14.57ms; iter 97800: train loss 0.00006\n",
      "iter_dt 15.93ms; iter 97900: train loss 0.00016\n",
      "iter_dt 16.27ms; iter 98000: train loss 0.00003\n",
      "iter_dt 16.23ms; iter 98100: train loss 0.00023\n",
      "iter_dt 12.73ms; iter 98200: train loss 0.00010\n",
      "iter_dt 17.15ms; iter 98300: train loss 0.00006\n",
      "iter_dt 12.56ms; iter 98400: train loss 0.00002\n",
      "iter_dt 15.67ms; iter 98500: train loss 0.00021\n",
      "iter_dt 12.61ms; iter 98600: train loss 0.00009\n",
      "iter_dt 13.71ms; iter 98700: train loss 0.00005\n",
      "iter_dt 12.73ms; iter 98800: train loss 0.00001\n",
      "iter_dt 13.74ms; iter 98900: train loss 0.00007\n",
      "iter_dt 12.48ms; iter 99000: train loss 0.00002\n",
      "iter_dt 52.32ms; iter 99100: train loss 0.00007\n",
      "iter_dt 30.43ms; iter 99200: train loss 0.00008\n",
      "iter_dt 33.88ms; iter 99300: train loss 0.00009\n",
      "iter_dt 24.54ms; iter 99400: train loss 0.00004\n",
      "iter_dt 21.23ms; iter 99500: train loss 0.00010\n",
      "iter_dt 56.22ms; iter 99600: train loss 0.00001\n",
      "iter_dt 15.08ms; iter 99700: train loss 0.00017\n",
      "iter_dt 21.25ms; iter 99800: train loss 0.00002\n",
      "iter_dt 13.31ms; iter 99900: train loss 0.00013\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# This function is called at the end of every batch in training\n",
    "# and is used to report the amount of time per 100 batches, and the loss at that point\n",
    "loss_train=np.zeros(trainer.config.max_iters)\n",
    "loss_val=np.zeros(trainer.config.max_iters)\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "    loss_train[trainer.iter_num]=trainer.loss.item()\n",
    "    loss_val[trainer.iter_num]=trainer.val_loss.item()\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "# Train!\n",
    "trainer.run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:52:19.072616200Z",
     "start_time": "2023-10-23T02:27:08.134329Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.7041666666666667\n",
      "[0.00290389]\n",
      "[0.29625264]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#smoothing the curve\n",
    "for i in range (trainer.config.max_iters-1):\n",
    "    loss_train[i+1]=loss_train[i]*0.99+loss_train[i+1]*0.01\n",
    "    loss_val[i+1]=loss_val[i]*0.99+loss_val[i+1]*0.01\n",
    "\n",
    "\n",
    "#plotting section\n",
    "fig, splot = plt.subplots(1)\n",
    "domain = np.arange(trainer.config.max_iters)\n",
    "splot.plot(domain, loss_train, 'g',label=\"train\")\n",
    "splot.plot(domain, loss_val, 'r',label=\"validation\")\n",
    "splot.legend()\n",
    "splot.title.set_text(\"Loss\")\n",
    "correct=0\n",
    "for i in range (len(val_dataset)):\n",
    "    x,y = val_dataset[i]\n",
    "    generated_logits = trainer.model(x[None,:].to(trainer.device), finetune_classify=True)\n",
    "    pred=(generated_logits[0][0][0]>generated_logits[0][0][1]).item()\n",
    "    target=(y[0]>y[1]).item()\n",
    "    if pred==target:\n",
    "        correct+=1\n",
    "\n",
    "print(\"Final test accuracy:\",correct/len(val_dataset))\n",
    "print(loss_train[-1:])\n",
    "print(loss_val[-1:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:52:22.251200800Z",
     "start_time": "2023-10-23T02:52:19.084831400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "model.to(trainer.device)\n",
    "# store the saved model in a file, so can re-use later\n",
    "modelsavename= \"mint_classifier.pt\"  # change the name here to save in a specific file (and restore below)\n",
    "with open(modelsavename, \"wb\") as f:\n",
    "    torch.save(trainer.model.state_dict(), f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:10:15.725950Z",
     "start_time": "2023-10-23T02:10:15.665752400Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c245645368b405f9e41f3dedb59d0df7c5d5feced548513488e8eb3fe8134cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
