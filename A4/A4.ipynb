{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T05:50:19.193495900Z",
     "start_time": "2023-11-10T05:50:18.568641900Z"
    }
   },
   "id": "66bc49d816b9c4bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "750c512e92a3b884"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T02:47:58.302068Z",
     "start_time": "2023-11-09T02:47:54.322681400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[65], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m (\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m (\u001B[38;5;241m8\u001B[39m):\n\u001B[1;32m---> 14\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(input_ids, do_sample\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, temperature \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m11.6\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m0.75\u001B[39m\u001B[38;5;241m*\u001B[39mi,top_p\u001B[38;5;241m=\u001B[39m(j\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m30\u001B[39m,repetition_penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.1\u001B[39m)\n\u001B[0;32m     15\u001B[0m         list_out\u001B[38;5;241m.\u001B[39mappend(outputs)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m (\u001B[38;5;241m1\u001B[39m):\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\utils.py:1642\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1634\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   1635\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1636\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   1637\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   1638\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   1639\u001B[0m     )\n\u001B[0;32m   1641\u001B[0m     \u001B[38;5;66;03m# 13. run sample\u001B[39;00m\n\u001B[1;32m-> 1642\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample(\n\u001B[0;32m   1643\u001B[0m         input_ids,\n\u001B[0;32m   1644\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[0;32m   1645\u001B[0m         logits_warper\u001B[38;5;241m=\u001B[39mlogits_warper,\n\u001B[0;32m   1646\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[0;32m   1647\u001B[0m         pad_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mpad_token_id,\n\u001B[0;32m   1648\u001B[0m         eos_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39meos_token_id,\n\u001B[0;32m   1649\u001B[0m         output_scores\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39moutput_scores,\n\u001B[0;32m   1650\u001B[0m         return_dict_in_generate\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mreturn_dict_in_generate,\n\u001B[0;32m   1651\u001B[0m         synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[0;32m   1652\u001B[0m         streamer\u001B[38;5;241m=\u001B[39mstreamer,\n\u001B[0;32m   1653\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   1654\u001B[0m     )\n\u001B[0;32m   1656\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH:\n\u001B[0;32m   1657\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[0;32m   1658\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[0;32m   1659\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   1660\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1665\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[0;32m   1666\u001B[0m     )\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\utils.py:2724\u001B[0m, in \u001B[0;36mGenerationMixin.sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[0;32m   2721\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[0;32m   2723\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[1;32m-> 2724\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(\n\u001B[0;32m   2725\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs,\n\u001B[0;32m   2726\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   2727\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m   2728\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[0;32m   2729\u001B[0m )\n\u001B[0;32m   2731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[0;32m   2732\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1076\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1068\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1069\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[0;32m   1071\u001B[0m \u001B[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[0;32m   1072\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1074\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1076\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(\n\u001B[0;32m   1077\u001B[0m     input_ids,\n\u001B[0;32m   1078\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[0;32m   1079\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   1080\u001B[0m     token_type_ids\u001B[38;5;241m=\u001B[39mtoken_type_ids,\n\u001B[0;32m   1081\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   1082\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[0;32m   1083\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[0;32m   1084\u001B[0m     encoder_hidden_states\u001B[38;5;241m=\u001B[39mencoder_hidden_states,\n\u001B[0;32m   1085\u001B[0m     encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_attention_mask,\n\u001B[0;32m   1086\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[0;32m   1087\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m   1088\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[0;32m   1089\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1090\u001B[0m )\n\u001B[0;32m   1091\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1093\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:900\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    890\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    891\u001B[0m         create_custom_forward(block),\n\u001B[0;32m    892\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    897\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    898\u001B[0m     )\n\u001B[0;32m    899\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 900\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m block(\n\u001B[0;32m    901\u001B[0m         hidden_states,\n\u001B[0;32m    902\u001B[0m         layer_past\u001B[38;5;241m=\u001B[39mlayer_past,\n\u001B[0;32m    903\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m    904\u001B[0m         head_mask\u001B[38;5;241m=\u001B[39mhead_mask[i],\n\u001B[0;32m    905\u001B[0m         encoder_hidden_states\u001B[38;5;241m=\u001B[39mencoder_hidden_states,\n\u001B[0;32m    906\u001B[0m         encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_attention_mask,\n\u001B[0;32m    907\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[0;32m    908\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m    909\u001B[0m     )\n\u001B[0;32m    911\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:390\u001B[0m, in \u001B[0;36mGPT2Block.forward\u001B[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[0;32m    388\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m    389\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(hidden_states)\n\u001B[1;32m--> 390\u001B[0m attn_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(\n\u001B[0;32m    391\u001B[0m     hidden_states,\n\u001B[0;32m    392\u001B[0m     layer_past\u001B[38;5;241m=\u001B[39mlayer_past,\n\u001B[0;32m    393\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m    394\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[0;32m    395\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[0;32m    396\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m    397\u001B[0m )\n\u001B[0;32m    398\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# output_attn: a, present, (attentions)\u001B[39;00m\n\u001B[0;32m    399\u001B[0m outputs \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:334\u001B[0m, in \u001B[0;36mGPT2Attention.forward\u001B[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[0;32m    331\u001B[0m     attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_attn(query, key, value, attention_mask, head_mask)\n\u001B[0;32m    333\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_heads(attn_output, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n\u001B[1;32m--> 334\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_proj(attn_output)\n\u001B[0;32m    335\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresid_dropout(attn_output)\n\u001B[0;32m    337\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (attn_output, present)\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\Anaconda\\Lib\\site-packages\\transformers\\pytorch_utils.py:105\u001B[0m, in \u001B[0;36mConv1D.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    104\u001B[0m     size_out \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnf,)\n\u001B[1;32m--> 105\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39maddmm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, x\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight)\n\u001B[0;32m    106\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(size_out)\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# sample up to 30 tokens\n",
    "torch.manual_seed(11)\n",
    "list_out=[]\n",
    "for i in range (1):\n",
    "    for j in range (8):\n",
    "        outputs = model.generate(input_ids, do_sample=True, max_length=30, temperature =11.6+0.75*i,top_p=(j+1)/30,repetition_penalty=1.1)\n",
    "        list_out.append(outputs)\n",
    "\n",
    "for i in range (1):\n",
    "    for j in range (8):\n",
    "        print(\"temperature =\",11.6+0.75*i,\"top_p=\",(j+1)/30)\n",
    "        print(tokenizer.batch_decode(list_out[i*4+j], skip_special_tokens=True))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Show Probability"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5456b750ac10c857"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n"
     ]
    },
    {
     "data": {
      "text/plain": "           token    string  probability(%)\n0    tensor(340)        it       16.150859\n1    tensor(318)        is       33.391905\n2    tensor(257)         a       15.054569\n3   tensor(1994)       key        6.984391\n4    tensor(636)      part       15.045089\n5    tensor(286)        of       98.910081\n6    tensor(262)       the       26.422513\n7   tensor(3298)    global       11.689099\n8   tensor(3773)   economy       19.673710\n9     tensor(13)         .       33.241472\n10   tensor(198)        \\n       21.853313\n11   tensor(198)        \\n       99.950582\n12     tensor(1)         \"       28.311592\n13  tensor(1135)        We       14.028271\n14   tensor(761)      need       23.630184\n15   tensor(284)        to       78.723419",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>string</th>\n      <th>probability(%)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tensor(340)</td>\n      <td>it</td>\n      <td>16.150859</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tensor(318)</td>\n      <td>is</td>\n      <td>33.391905</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tensor(257)</td>\n      <td>a</td>\n      <td>15.054569</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tensor(1994)</td>\n      <td>key</td>\n      <td>6.984391</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tensor(636)</td>\n      <td>part</td>\n      <td>15.045089</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>tensor(286)</td>\n      <td>of</td>\n      <td>98.910081</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>tensor(262)</td>\n      <td>the</td>\n      <td>26.422513</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>tensor(3298)</td>\n      <td>global</td>\n      <td>11.689099</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>tensor(3773)</td>\n      <td>economy</td>\n      <td>19.673710</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>tensor(13)</td>\n      <td>.</td>\n      <td>33.241472</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>tensor(198)</td>\n      <td>\\n</td>\n      <td>21.853313</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>tensor(198)</td>\n      <td>\\n</td>\n      <td>99.950582</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>tensor(1)</td>\n      <td>\"</td>\n      <td>28.311592</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>tensor(1135)</td>\n      <td>We</td>\n      <td>14.028271</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>tensor(761)</td>\n      <td>need</td>\n      <td>23.630184</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>tensor(284)</td>\n      <td>to</td>\n      <td>78.723419</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_ids, do_sample=False, num_beams=1, max_length=30, temperature =1,top_p=0.99,return_dict_in_generate=True, output_scores=True,repetition_penalty=1.0)\n",
    "print(tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True))\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1] #?\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "df=pd.DataFrame(columns=[\"token\",\"string\",\"probability(%)\"])\n",
    "\n",
    "i=0\n",
    "for score in transition_scores[0]:\n",
    "    tok=generated_tokens[0,i]\n",
    "    # | token | token string | logits | probability\n",
    "    #print(f\"| {tok:5d} {str(tokenizer.batch_decode(generated_tokens[:,i], skip_special_tokens=True)):12s} {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
    "    df.loc[i] = [tok, tokenizer.decode(generated_tokens[:,i]), 100*np.exp(score.numpy())]\n",
    "    i+=1\n",
    "    \n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T06:19:54.057355800Z",
     "start_time": "2023-11-10T06:19:53.544942300Z"
    }
   },
   "id": "5a91d1448122353e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate probability tree"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bec6b060ad9df26d"
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences= <class 'torch.Tensor'> 30\n",
      "['It is important for all countries to try harder to reduce carbon emissions because it is the only way to reduce global warming,\" he said.\\n\\n\"']\n",
      "[' because']\n",
      "├── [' it'] 16.0%\n",
      "│   ├── [' can'] 4.0%\n",
      "│   ├── [' is'] 33.0%\n",
      "│   │   ├── [' a'] 15.0%\n",
      "│   │   ├── [' not'] 5.0%\n",
      "│   │   └── [' the'] 11.0%\n",
      "│   │       ├── [' key'] 7.0%\n",
      "│   │       ├── [' major'] 7.0%\n",
      "│   │       ├── [' only']\n",
      "│   │       │   ├── [' option'] 2.0%\n",
      "│   │       │   ├── [' thing'] 2.0%\n",
      "│   │       │   └── [' way'] 70.0%\n",
      "│   │       │       ├── [' of'] 4.0%\n",
      "│   │       │       ├── [' to'] 73.0%\n",
      "│   │       │       │   ├── [' achieve'] 6.0%\n",
      "│   │       │       │   ├── [' prevent'] 5.0%\n",
      "│   │       │       │   └── [' reduce'] 17.0%\n",
      "│   │       │       │       ├── [' emissions'] 11.0%\n",
      "│   │       │       │       ├── [' global'] 9.0%\n",
      "│   │       │       │       │   ├── [' amount'] 5.0%\n",
      "│   │       │       │       │   ├── [' global'] 7.0%\n",
      "│   │       │       │       │   ├── [' warming']\n",
      "│   │       │       │       │   │   ├── [',\"'] 29.0%\n",
      "│   │       │       │       │   │   │   ├── [' he'] 21.0%\n",
      "│   │       │       │       │   │   │   │   ├── [' Dr'] 4.0%\n",
      "│   │       │       │       │   │   │   │   ├── [' Mr'] 1.0%\n",
      "│   │       │       │       │   │   │   │   ├── [' said']\n",
      "│   │       │       │       │   │   │   │   │   ├── ['\"'] 19.0%\n",
      "│   │       │       │       │   │   │   │   │   ├── ['.']\n",
      "│   │       │       │       │   │   │   │   │   │   ├── [' \"'] 19.0%\n",
      "│   │       │       │       │   │   │   │   │   │   ├── ['<|endoftext|>'] 7.0%\n",
      "│   │       │       │       │   │   │   │   │   │   └── ['\\n'] 63.0%\n",
      "│   │       │       │       │   │   │   │   │   │       ├── [','] 0.0%\n",
      "│   │       │       │       │   │   │   │   │   │       ├── ['.'] 0.0%\n",
      "│   │       │       │       │   │   │   │   │   │       └── ['\\n'] 100.0%\n",
      "│   │       │       │       │   │   │   │   │   │           ├── ['\"'] 21.0%\n",
      "│   │       │       │       │   │   │   │   │   │           ├── ['He'] 5.0%\n",
      "│   │       │       │       │   │   │   │   │   │           └── ['The'] 13.0%\n",
      "│   │       │       │       │   │   │   │   │   ├── ['The'] 10.0%\n",
      "│   │       │       │       │   │   │   │   │   └── ['We'] 2.0%\n",
      "│   │       │       │       │   │   │   │   └── [' the'] 3.0%\n",
      "│   │       │       │       │   │   │   ├── [' said'] 23.0%\n",
      "│   │       │       │       │   │   │   └── [' she'] 4.0%\n",
      "│   │       │       │       │   │   ├── [','] 8.0%\n",
      "│   │       │       │       │   │   └── ['.'] 26.0%\n",
      "│   │       │       │       │   └── [' world'] 6.0%\n",
      "│   │       │       │       └── [' the'] 18.0%\n",
      "│   │       │       └── [' we'] 7.0%\n",
      "│   │       └── [' very'] 6.0%\n",
      "│   └── [' will'] 21.0%\n",
      "├── [' the'] 8.0%\n",
      "└── [' they'] 8.0%\n"
     ]
    }
   ],
   "source": [
    "from treelib import Node, Tree\n",
    "\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# sample up to 30 tokens\n",
    "torch.manual_seed(11)\n",
    "outputs = model.generate(input_ids, do_sample=False, num_beams=3, max_length=30, temperature =1,top_p=0.99,return_dict_in_generate=True, output_scores=True,repetition_penalty=1.0)\n",
    "sequences = outputs.sequences # sequences is a tensor of shape (batch_size, sequence_length)\n",
    "scores = outputs.scores\n",
    "print(\"sequences=\",type(sequences),len(sequences[0]))\n",
    "print(tokenizer.batch_decode(sequences, skip_special_tokens=True))\n",
    "\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "node=0\n",
    "tree.create_node(str(tokenizer.batch_decode(sequences[0:1,13], skip_special_tokens=True)), node)  # root node\n",
    "parent=node\n",
    "\n",
    "i=0\n",
    "for score in scores:\n",
    "    token_num=sequences[0,i+14]\n",
    "     \n",
    "    \n",
    "    tok=torch.argmax(score[0])\n",
    "    prob=np.round(100*np.exp(max(score[0]).numpy()))\n",
    "    string=str([tokenizer.decode(tok)])+\" \"+str(prob)+\"%\"\n",
    "    score[0][torch.argmax(score[0])]-=100\n",
    "    node+=1\n",
    "    if tok==token_num:\n",
    "        parent_next=node\n",
    "    tree.create_node(string, node ,parent=parent)\n",
    "    \n",
    "    \n",
    "    tok=torch.argmax(score[0])\n",
    "    prob=np.round(100*np.exp(max(score[0]).numpy()))\n",
    "    string=str([tokenizer.decode(tok)])+\" \"+str(prob)+\"%\"\n",
    "    score[0][torch.argmax(score[0])]-=100\n",
    "    node+=1\n",
    "    if tok==token_num:\n",
    "        parent_next=node\n",
    "    tree.create_node(string, node ,parent=parent)\n",
    "\n",
    "    tok=torch.argmax(score[0])\n",
    "    prob=np.round(100*np.exp(max(score[0]).numpy()))\n",
    "    string=str([tokenizer.decode(tok)])+\" \"+str(prob)+\"%\"\n",
    "    score[0][torch.argmax(score[0])]-=100\n",
    "    node+=1\n",
    "    if tok==token_num:\n",
    "        parent_next=node\n",
    "    tree.create_node(string, node ,parent=parent)\n",
    "    \n",
    "    if parent==parent_next:\n",
    "        node+=1\n",
    "        parent_next=node\n",
    "        tree.create_node(str(tokenizer.batch_decode(sequences[0:1,i+14])), node ,parent=parent)\n",
    "        \n",
    "    parent=parent_next\n",
    "    i+=1\n",
    "\n",
    "tree.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T21:23:17.773234200Z",
     "start_time": "2023-11-10T21:23:09.277494300Z"
    }
   },
   "id": "62e1ea4ee2898c73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# P4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d000b9f5e6286f0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you might be feeling a little concerned that smoking could be gradually affecting your wellbeing.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key =\"\")\n",
    "\n",
    "sys_prom=\"You are a kind therapist, skilled in explaining cold matter in a warm way.\"\n",
    "usr_prom=\"Turn the sentence \\\"You dislike the fact that your health is slowly deteriorating after each cigarette.\\\" into one that's softened, and non-expert so that it doesn't sound as authoritative nor certain. Generate 3 samples\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\":sys_prom },\n",
    "        {\"role\": \"user\", \"content\":usr_prom }\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T19:42:07.354593100Z",
     "start_time": "2023-11-11T19:42:06.071778400Z"
    }
   },
   "id": "f826f189ccb3b393"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. It seems like you're not fully enjoying the way your wellbeing gently changes with every cigarette.\n",
    "\n",
    "2. It might be that you're noticing a shift in your overall wellness, seemingly connected with each smoke?\n",
    "\n",
    "3. It seems like you might be feeling a little concerned that smoking could be gradually affecting your wellbeing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76a08260bb75fda"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"1. It seems like you're not fully enjoying the way your wellbeing gently changes with every cigarette.\\n\\n2. It might be that you're noticing a shift in your overall wellness, seemingly connected with each smoke?\\n\\n3. You're possibly feeling a bit\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T19:41:10.356230700Z",
     "start_time": "2023-11-11T19:41:10.350488300Z"
    }
   },
   "id": "245c2174032281ba"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seemed like you were going through a tough time.\n"
     ]
    }
   ],
   "source": [
    "sys_prom=\"You are a kind therapist, skilled in explaining cold matter in a warm way.\"\n",
    "usr_prom=\"Turn the sentence \\\"You were in a lot of pain.\\\" into one that's softened, and non-expert so that it doesn't sound as authoritative nor certain. Generate 1 samples\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\":sys_prom },\n",
    "        {\"role\": \"user\", \"content\":usr_prom }\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T20:21:13.951769100Z",
     "start_time": "2023-11-11T20:21:12.362796600Z"
    }
   },
   "id": "500621c4f195a9af"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you might find some comfort in the soothing sensation that nicotine seems to provide you.\n",
      "It seems like you might be reaching for the amphetamines a bit more than is usually suggested.\n",
      "It seems like the scent of cigarette smoke might not be your favorite.\n",
      "So, you've come today to share a bit about your experiences with gambling, haven't you?\n",
      "You're beginning to see that by tweaking a few things, there could be some pretty significant shifts in your everyday world, and that's got you feeling a tad bit excited about all the possibilities, doesn't it?\n"
     ]
    }
   ],
   "source": [
    "f = open(\"DirectStatements.csv\", \"r\",encoding='utf-8-sig')\n",
    "sentences=[]\n",
    "for i in f.readlines():\n",
    "    sentences.append(i.strip())\n",
    "   \n",
    "for i in range(2,7,1):\n",
    "    usr_prom=\"Turn the sentence \\\"\"+sentences[i]+\"\\\" into one that's softened, and non-expert so that it doesn't sound as authoritative nor certain. Generate 1 samples\"\n",
    "    #print(usr_prom)\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":sys_prom },\n",
    "            {\"role\": \"user\", \"content\":usr_prom }\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    print(completion.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T21:24:52.143597600Z",
     "start_time": "2023-11-11T21:24:45.389430600Z"
    }
   },
   "id": "89ae53e9688162df"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key =\"\")\n",
    "\n",
    "sys_prom=\"You are a kind therapist, skilled in explaining cold matter in a warm way.\"\n",
    "\n",
    "f = open(\"DirectStatements.csv\", \"r\",encoding='utf-8-sig')\n",
    "fo=open(\"A4_4_4.csv\", \"w\")\n",
    "\n",
    "sentences=[]\n",
    "for i in f.readlines():\n",
    "    sentences.append(i.strip())\n",
    "\n",
    "for i in range(7,9,1):\n",
    "    usr_prom=\"\\\"Turn the sentence \\\"\\\"\"+sentences[i]+\"\\\"\\\" into one that's softened, and non-expert so that it doesn't sound as authoritative nor certain. Generate 1 samples\\\"\"\n",
    "    fo.write(sentences[i])\n",
    "    fo.write(\",\\\"\")\n",
    "    fo.write(sentences[i])\n",
    "    fo.write(\"\\\",\\n\")\n",
    "    \n",
    "f.close()\n",
    "fo.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T21:33:15.355598Z",
     "start_time": "2023-11-11T21:33:15.337359300Z"
    }
   },
   "id": "ae591837c4d58c54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# P5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb698e94ece8566"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sys_prom=\"You are a kind therapist, skilled in explaining cold matter in a warm way.\"\n",
    "usr_prom=(\"You will be given a fen sentences and classify them based on softness. Here softness refers to whether a sentence is authoritative and direct, or is expressed as a suggestion or question. If a sentence is hard, print 0, and if it's soft, print 1. Here are the sentences: You dislike that others know that you smoke.\\nYou want to stop smoking cigars altogether.\\nYou seem low-energy.\\nIt seems like you might be feeling a little concerned that smoking could be gradually affecting your wellbeing.\\nIt seemed like you were going through a tough time.\\nIt seems like you might find some comfort in the soothing sensation that nicotine seems to provide you.\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\":sys_prom },\n",
    "        {\"role\": \"user\", \"content\":usr_prom }\n",
    "    ],\n",
    "    max_tokens=200\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T00:58:27.032995700Z",
     "start_time": "2023-11-12T00:58:26.253489Z"
    }
   },
   "id": "7481e0d1fa7940bf"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys_prom=\"You are a kind therapist, skilled in explaining cold matter in a warm way.\"\n",
    "usr_prom=\"You will be given a fen sentences and classify them based on softness. Here softness refers to whether a sentence is authoritative, direct, stating, or is expressed as a suggestion or question. If a sentence is hard, print 0, and if it's soft, print 1. Here are the sentences:\\n\"\n",
    "\n",
    "\n",
    "f = open(\"A4_5_1.csv\", \"r\",encoding='utf-8-sig')\n",
    "\n",
    "sentences=[]\n",
    "for i in f.readlines():\n",
    "    usr_prom=usr_prom+i.replace(\"0\",\"\").replace(\"1\",\"\")+\"\\n\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\":sys_prom },\n",
    "        {\"role\": \"user\", \"content\":usr_prom }\n",
    "    ],\n",
    "    max_tokens=200\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T03:50:25.428973500Z",
     "start_time": "2023-11-12T03:50:17.163982700Z"
    }
   },
   "id": "695ceeba44f5b1d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b5387ac77389d05d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# P6"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8871877724bf347d"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 0 - This sentence is direct and specific about the person's health issue due to smoking, leaving little room for interpretation or exploration.\n",
      "2. 0 - This statement asserts a fact about the person's past pain without any room for exploration or interpretation.\n",
      "3. 0 - This statement asserts a fact about a person's enjoyment without room for exploration or nuanced interpretation.\n",
      "4. 0 - This statement is making a direct judgement about a person's drug use without room for interpretation or their own evaluation.\n",
      "5. 0 - This statement directly states the person's dislike, leaving no room for their own interpretation or exploration.\n",
      "6. 1 - This sentence is soft as it's a question that softly inquires about the person's reason for the visit, leaving their response open-ended.\n",
      "7. 0 - This sentence is hard because it assumes very specific feelings and expectations about the person's life changes.\n",
      "8. 0 - This statement directly asserts a person's concern without any room for interpretation or exploration.\n",
      "9. 1 - This sentence is soft as it points towards a person's beliefs and is left open to interpretation or correction.\n",
      "10. 0 - This statement assumes specific intentions and methods for addressing a person's health issue without leaving room for exploration or discussion.\n",
      "11. 0 - This specifies why a person enjoys smoking without their own input or interpretation.\n",
      "12. 0 - This statement outrightly assumes the person's reaction to bad treatment, leaving no room for their own input or exploration.\n",
      "13. 0 - This statement is giving a directive without the person's input, leaving no room for nuanced discussion.\n",
      "14. 0 - This statement assumes a person's financial struggle due to smoking, not leaving an opening for personal input.\n",
      "15. 0 - This statement directly asserts a person's feelings about smoking's health risks without leaving room for discussion.\n",
      "16. 0 - This statement assumes quitting method without leaving room for interpretation, thus it's hard.\n",
      "17. 0 - A hard statement as it directly associates cigarettes with a traumatic health event.\n",
      "18. 0 - This statement gives a directive about handling their smoking habit without the person's input.\n",
      "19. 1 - This is a softer way of suggesting that quitting smoking might be beneficial for the person's health.\n",
      "20. 0 - This statement directly asserts a person's feelings about social interaction without room for their own input.\n",
      "21. 0 - It imposes the speaker's view about productivity improvement on the listener, thus hard.\n",
      "22. 0 - This is a hard statement as it's straight-up directive for creating a stop-drinking plan.\n",
      "23. 0 - This statement presumes a person's desires about their smoking habit without leaving room for personal input.\n",
      "24. 1 - This sentence is a soft suggestion, asking the person to consider the benefits of making long-term plans.\n",
      "25. 0 - This is a hard assertion about the person's motive for staying in a relationship.\n",
      "26. 0 - This statement does not ask for the person's input or exploration of why they might be drinking more.\n",
      "27. 0 - This statement directly brings up a person's fear associated with smoking without their input.\n",
      "28. 0 - This statement does not leave room for a person's input on their concerns about others knowing they smoke.\n",
      "29. 0 - This statement directs a person to stop a specific habit without their input.\n",
      "30. 0 - This statement assumes the person's energy level without asking them directly.\n",
      "31. 1 - This statement gently suggests a possible concern and leaves room for the person to form their own interpretation.\n",
      "32. 1 - This sentence is soft as it gently points towards a person's difficult time, leaving room for their personal input.\n",
      "33. 1 - The use of \"might\" suggests a possibility, nudging the listener to think about the soothing sensation of nicotine.\n",
      "34. 1 - The use of \"seems\" and \"might\" make this a soft suggestion about the listener's use of amphetamines.\n",
      "35. 1 - This statement is gentle, suggesting the person may not like the cigarette smoke scent, leaving room for their own input.\n",
      "36. 1 - This sentence is a soft way of asking about a person's experience with gambling.\n",
      "37. 1 - This sentence softly suggests the potential positive effects of change and prompts for the listener's feelings.\n",
      "38. 1 - The sentence suggests the listener's thoughts without imposing any assertions, thus it's soft.\n",
      "39. 1 - This sentence suggests a possible change for health improvement in a gentle and supportive tone.\n",
      "40. 1 - This sentence softly suggests the listener's thoughts and feelings about quitting smoking and presents a solution.\n",
      "41. 1 - This sentence is soft as it's a gentle inquiry about the listener's stress coping mechanism, opening up room for discussion.\n",
      "42. 1 - This sentence softly suggests the person's feelings in a past situation and respects their decision in a non-confrontational manner.\n",
      "43. 1 - This sentence is a gentle suggestion for the listener to consider a potential beneficial self-care option.\n",
      "44. 1 - This sentence softly suggests the listener's realization about their spending habits on cigarettes.\n",
      "45. 1 - This sentence is a gentle indication of the person's concern, leaving room for their own input.\n",
      "46. 1 - This sentence softly suggests a possible method for dealing with cravings after stopping smoking.\n",
      "47. 1 - This sentence gently reminds the listener of a significant past event in a sensitive way.\n",
      "48. 1 - This sentence softly makes a suggestion for planning about smoking habits, giving the listener the space to consider or deny it.\n",
      "49. 1 - The use of \"Have you ever thought about\" makes this sentence a soft suggestion considering the positive effect of quitting smoking.\n",
      "50. 1 - This sentence is a soft exploration of the listener's feelings and gently offers support by understanding their social anxiety.\n",
      "51. 1 - This sentence gently suggests a potential way for improving productivity and seeks for listener's consent.\n",
      "52. 1 - This sentence gently suggests the listener consider reducing their alcohol intake.\n",
      "53. 1 - The sentence suggests that setting a routine might be the key, but leaves room for the listener's interpretation.\n",
      "54. 1 - This sentence gently encourages the listener to think about setting long-term goals and asks if they agree.\n",
      "55. 1 - This sentence gently suggests that the listener might be holding onto a relationship due to fear, encouraging them to think about it.\n",
      "56. 1 - This sentence softly suggests that the listener might be drinking more when alone, encouraging them to reflect on that.\n",
      "57. 1 - This sentence gently suggests the person's discomfort associated with their smoking habit.\n",
      "58. 1 - This sentence softly mentions the listener's discomfort when others know about their smoking habit.\n",
      "59. 1 - This sentence gently suggests that the listener might be considering quitting cigars, rather than imposing it as a fact.\n",
      "60. 1 - This sentence uses mild language to suggest the listener might be feeling less energetic, leaving space for their interpretation.\n"
     ]
    }
   ],
   "source": [
    "sys_prom=\"You are a kind therapist, skilled in explaining cold matter in a warm way.\"\n",
    "usr_prom=(\"You will be given a few sentences and classify them based on softness. Here softness refers to whether a sentence is authoritative and direct, stating some facts in an unquestionable manner; or is expressed as a gentle suggestion for possibility or a question where the person speaking sounds as if they do not know the other party better than the other party themselves. Also keep in mind that just because a sentence is a question etc. doesn't mean it's soft. The subtlety of the question on sensitive topics e.g. wellbeing, social relations, self-confidence etc. must also be taken into consideration. For each sentence, explain why is it a soft/hard sentence instead of the other. If a sentence is hard, print 0, and if it's soft, print 1. Here are the sentences:\\n\")\n",
    "\n",
    "f = open(\"A4_5_1.csv\", \"r\",encoding='utf-8-sig')\n",
    "\n",
    "sentences=[]\n",
    "for i in f.readlines():\n",
    "    usr_prom=usr_prom+i.replace(\"0\",\"\").replace(\"1\",\"\")+\"\\n\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\":sys_prom },\n",
    "        {\"role\": \"user\", \"content\":usr_prom }\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T04:08:57.246187Z",
     "start_time": "2023-11-12T04:07:46.550349400Z"
    }
   },
   "id": "3c3aa6dbf3784872"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
